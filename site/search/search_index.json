{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"whyqd: simplicity, transparency, speed","text":""},{"location":"#what-is-it","title":"What is it?","text":"<p>More research, less wrangling</p> <p>whyqd (/w\u026ak\u026ad/) is a curatorial toolkit intended to produce well-structured and predictable  data for research analysis.</p> <p>It provides an intuitive method for creating schema-to-schema crosswalks for restructuring messy data to conform to a  standardised metadata schema. It supports rapid and continuous transformation of messy data using a simple series of  steps. Once complete, you can import wrangled data into more complex analytical or database systems.</p> <p>whyqd plays well with your existing Python-based data-analytical tools. It uses Ray and  Modin as a drop-in replacement for Pandas to support  processing of large datasets, and Pydantic for data models. </p> <p>Each definition is saved as JSON Schema-compliant file. This permits others to read and  scrutinise your approach, validate your methodology, or even use your crosswalks to import and transform data in  production.</p> <p>Once complete, a transform file can be shared, along with your input data, and anyone can import and validate your  crosswalk to verify that your output data is the product of these inputs.</p>"},{"location":"#why-use-it","title":"Why use it?","text":"<p>If all you want to do is test whether your source data are even useful, spending days or weeks slogging through data  restructuring could kill a project. If you already have a workflow and established software which includes Python and  pandas, having to change your code every time your source data changes is really, really frustrating.</p> <p>If you want to go from a Cthulhu dataset like this:</p> <p></p> <p>UNDP Human Development Index 2007-2008: a beautiful example of messy data.</p> <p>To this:</p> country_name indicator_name reference year values 0 Hong Kong, China (SAR) HDI rank e 2008 21 1 Singapore HDI rank nan 2008 25 2 Korea (Republic of) HDI rank nan 2008 26 3 Cyprus HDI rank nan 2008 28 4 Brunei Darussalam HDI rank nan 2008 30 5 Barbados HDI rank e,g,f 2008 31 <p>With a readable set of scripts to ensure that your process can be audited and repeated:</p> <pre><code>schema_scripts = [\n    f\"UNITE &gt; 'reference' &lt; {REFERENCE_COLUMNS}\",\n    \"RENAME &gt; 'country_name' &lt; ['Country']\",\n    \"PIVOT_LONGER &gt; ['indicator_name', 'values'] &lt; ['HDI rank', 'HDI Category', 'Human poverty index (HPI-1) - Rank;;2008', 'Human poverty index (HPI-1) - Value (%);;2008', 'Probability at birth of not surviving to age 40 (% of cohort);;2000-05', 'Adult illiteracy rate (% aged 15 and older);;1995-2005', 'Population not using an improved water source (%);;2004', 'Children under weight for age (% under age 5);;1996-2005', 'Population below income poverty line (%) - $1 a day;;1990-2005', 'Population below income poverty line (%) - $2 a day;;1990-2005', 'Population below income poverty line (%) - National poverty line;;1990-2004', 'HPI-1 rank minus income poverty rank;;2008']\",\n    \"SEPARATE &gt; ['indicator_name', 'year'] &lt; ';;'::['indicator_name']\",\n    \"DEBLANK\",\n    \"DEDUPE\",\n]\n</code></pre> <p>There are two complex and time-consuming parts to preparing data for analysis: social, and technical.</p> <p>The social part requires multi-stakeholder engagement with source data-publishers, and with destination database users, to agree structural metadata. Without any agreement on data publication formats or destination structure, you are left with the tedious frustration of manually wrangling each independent dataset into a single schema.</p> <p>whyqd allows you to get to work without requiring you to achieve buy-in from anyone or change your existing code.</p>"},{"location":"#how-does-it-work","title":"How does it work?","text":"<p>Definition</p> <p>Crosswalks are mappings of the relationships between fields defined in different metadata  schemas. Ideally, these are one-to-one, where a field in one has an exact match in the other. In practice, it's more complicated than that.</p> <p>Your workflow is:</p> <ol> <li>Define a single destination schema,</li> <li>Derive a source schema from a data source,</li> <li>Review your source data structure,</li> <li>Develop a crosswalk to define the relationship between source and destination,</li> <li>Transform and validate your outputs,</li> <li>Share your output data, transform definitions, and a citation.</li> </ol> <p>It starts like this:</p> <pre><code>import whyqd as qd\n</code></pre> <p>Install and get started.</p> <p>Tutorials</p> <p>There are three worked tutorials to guide you through three typical scenarios:</p> <ul> <li>Aligning multiple data disparate sources to a single schema</li> <li>Pivoting wide-format data into archival long-format</li> <li>Wrangling Cthulhu data without losing your mind</li> </ul>"},{"location":"#licence","title":"Licence","text":"<p>The whyqd Python distribution is licensed under the terms of the  BSD 3-Clause license. All documentation is released under  Attribution 4.0 International (CC BY 4.0). whyqd tradenames and  marks are copyright Whythawk.</p>"},{"location":"changelog/","title":"Change log","text":""},{"location":"changelog/#version-100-2023-05-10","title":"Version 1.0.0 (2023-05-10)","text":"<p>This version shares some features with the previous version, but is a complete refactoring and conceptual redesign. It is not backwardly compatible. Future versions will maintain compatability with this one.</p> <ul> <li>Separated data models from schema models so that crosswalks are schema-to-schema.</li> <li>Complete revision of the API into four discrete <code>Definition</code> classes, <code>SchemaDefinition</code>, <code>DataSourceDefinition</code>,   <code>CrosswalkDefinition</code> and <code>TransformDefinition</code>.</li> <li>Removed <code>filters</code> and <code>actions</code> that are no longer relevant (including <code>REBASE</code>, and <code>MERGE</code>).</li> <li>Simplified <code>CATEGORISE</code> since it no longer requires deriving terms as part of the crosswalk.</li> <li>Crosswalks are designed to support continuous integration.</li> <li>Pydantic models are more transparent via each <code>Definition</code>'s <code>.get</code> property.</li> <li>Refactored Pandas to support Modin and Ray for data &gt;1 million rows.</li> <li>Mime type support for data sources in <code>Parquet</code> and <code>Feather</code>.</li> <li>Rewrote documentation in MKDocs from Sphinx.</li> <li>Revised all tutorials and documentation.</li> </ul>"},{"location":"changelog/#legacy-version-history","title":"Legacy version history","text":"<ul> <li>0.6.2: Fix for parsing ambiguity errors, plus Excel row-count exceeded on save.</li> <li>0.6.1: Minor correction for row count.</li> <li>0.6.0: Ensuring consistent optional column 'name' change for header row, plus new row-count in input and working data.</li> <li>0.5.0: Breaking API changes, including a complete rebuild to support Pydantic and type annotations.</li> <li>0.3.1: Minor documentation corrections &amp; updated Manifest.</li> <li>0.3.0: Introduced <code>Morph</code> table restructuring functions.</li> <li>0.2.0: Refactored <code>Action</code> wrangling functions to support new drop-in actions.</li> <li>0.1.1: Minor bug fixes.</li> <li>0.1.0: Initial release.</li> </ul>"},{"location":"contributing/","title":"Contributing to whyqd","text":"<p>We'd love you to support whyqd!</p>"},{"location":"contributing/#issues","title":"Issues","text":"<p>Questions, feature requests and bug reports are all welcome as  discussions or issues.</p> <p>Please include as much context as possible with your requests so others can replicate your problem, including any dependencies or data sources.</p> <p>To report a security vulnerability, please contact us so that we can get a fix out as quickly as possible.</p>"},{"location":"contributing/#pull-requests","title":"Pull requests","text":"<p>Before making a pull request, please create an issue to discuss your proposed change.</p> <p>There are a range of opportunities for getting into contributing, including:</p> <ul> <li>Expanding the documentation,</li> <li>Adding in additional 'action' scripts,</li> <li>Refactoring or optimising the code.</li> </ul> <p>Please ensure you run tests locally. whyqd tests don't require a database and have few dependencies, so running them doesn't take long and should be straightforward.</p> <p>If you add in new features, please also add in tests and documentation.</p> <p>Warning</p> <p>Keep focused. whyqd is a curatorial toolkit intended to produce well-structured and predictable data for  research analysis. It is not an analytical tool. Features related to data aggregation or analysis will not be considered.</p>"},{"location":"contributing/#financial-support","title":"Financial support","text":"<p>whyqd received initial funding from the European Union's Horizon 2020 research and innovation programme under grant agreement No 101017536. Technical  development support is from EOSC Future through the  RDA Open Call mechanism, based on evaluations of  external, independent experts.</p> <p>If you want to support us financially to ensure this project's continuity and development, please  contact us.</p>"},{"location":"installation/","title":"Installation and environment settings","text":"<p>whyqd (/w\u026ak\u026ad/) can be integrated into an existing data importer, or you could use it as part of your data analysis and exploration in Jupyter Notebooks.</p>"},{"location":"installation/#install","title":"Install","text":"<p>Install with your favourite package manager:</p> <pre><code>pip install whyqd\n</code></pre> <p>Then import in Python:</p> <pre><code>import whyqd as qd\n</code></pre>"},{"location":"installation/#settings","title":"Settings","text":"<p>whyqd uses Ray and Modin as a drop-in replacement for  Pandas to support processing of large datasets. This is less noticeable if you mostly  work with &lt;1m rows of data, but the power is there should you need it.</p> <p>The following can be set in your root <code>.env</code> project file:</p> <ul> <li><code>WHYQD_MEMORY</code>: the memory allocated for processing (default is 6Gb, as bytes <code>6000000000</code>).</li> <li><code>WHYQD_CPUS</code>: number of CPUS allocated for parallel processing (default is 3).</li> <li><code>WHYQD_SPILLWAY</code>: Ray will spill to local    storage when memory is exceeded. You can specify a temporary folder (default is \"/tmp/spill\"). This will be automatically   cleared every time whyqd is restarted or reinitialised.</li> <li><code>WHYQD_DIRECTORY</code>: a working directory for local storage (default is \"\").</li> <li><code>WHYQD_DEFAULT_MIMETYPE</code>: a default mime type for destination data (default output is    \"application/vnd.apache.parquet\").</li> </ul> <p>whyqd supports any of the following file mime types:</p> <ul> <li><code>CSV</code>: \"text/csv\"</li> <li><code>XLS</code>: \"application/vnd.ms-excel\"</li> <li><code>XLSX</code>: \"application/vnd.openxmlformats-officedocument.spreadsheetml.sheet\"</li> <li><code>PARQUET</code> (or <code>PRQ</code>): \"application/vnd.apache.parquet\"</li> <li><code>FEATHER</code> (or <code>FTR</code>): \"application/vnd.apache.feather\"</li> </ul> <p>Neither of Parquet or  Feather yet have official mimetypes, so this is what we're using  for now.</p>"},{"location":"quickstart/","title":"Quick start","text":"<p>whyqd (/w\u026ak\u026ad/) reduces crosswalks to a series of action scripts, each defining an individual step which must be  performed to restructure source- into a destination data.</p> <p>Your workflow is:</p> <ol> <li>Define a single destination schema,</li> <li>Derive a source schema from a data source,</li> <li>Review your source data structure,</li> <li>Develop a crosswalk to define the relationship between source and destination,</li> <li>Transform and validate your outputs,</li> <li>Share your output data, transform definitions, and a citation.</li> </ol> <p>Quick links</p> <ul> <li>Define a schema model</li> <li>Derive a schema from a datasource</li> <li>Define a crosswalk to link source to destination</li> <li>Process your transform</li> </ul>"},{"location":"quickstart/#define-a-schema-model","title":"Define a schema model","text":"<p>Assume you want you destination schema to conform to this table structure:</p> la_code ba_ref occupant_name postcode occupation_state occupation_state_date prop_ba_rates occupation_state_reliefs E06000044 177500080710 A company PO5 2SE True 2019-04-01 98530 [small_business, retail] <p>Define your model as follows:</p> <pre><code>import whyqd as qd\n\nschema: qd.models.SchemaModel = {\n    \"name\": \"rates_data\",\n    \"title\": \"Commercial rates data\",\n    \"description\": \"Standardised schema for archival and analysis of commercial / non-domestic rates data.\",\n}\nfields: list[qd.models.FieldModel] = [\n  {\n    \"name\": \"la_code\",\n    \"title\": \"Local authority code\",\n    \"type\": \"string\",\n    \"description\": \"Standard code for local authority.\"\n  },\n  {\n    \"name\": \"ba_ref\",\n    \"title\": \"Billing reference\",\n    \"type\": \"string\",\n    \"description\": \"Unique code for a specific hereditament. May be multiple rows for history.\"\n  },\n  {\n    \"name\": \"prop_ba_rates\",\n    \"title\": \"Property billing rates\",\n    \"type\": \"number\",\n    \"description\": \"Actual rates paid by a specific ratepayer.\"\n  },\n  {\n    \"name\": \"occupant_name\",\n    \"title\": \"Occupier name\",\n    \"type\": \"string\",\n    \"description\": \"Name of the ratepayer.\"\n  },\n  {\n    \"name\": \"postcode\",\n    \"title\": \"Postcode\",\n    \"type\": \"string\",\n    \"description\": \"Full address or postcode of ratepayer.\"\n  },\n  {\n    \"name\": \"occupation_state\",\n    \"title\": \"Occupation state\",\n    \"type\": \"boolean\",\n    \"description\": \"Occupation status, void or occupied.\"\n  },\n  {\n    \"name\": \"occupation_state_date\",\n    \"title\": \"Date of occupation state\",\n    \"type\": \"date\",\n    \"description\": \"Date of the start of status in occupation_state.\"\n  },\n  {\n    \"name\": \"occupation_state_reliefs\",\n    \"title\": \"Occupation state reliefs\",\n    \"type\": \"array\",\n    \"description\": \"Array of the categories of reliefs / exemptions applied.\"\n  }\n]\nschema_destination = qd.SchemaDefinition()\nschema_destination.set(schema=schema)\nschema_destination.fields.add_multi(terms=fields)\nschema_destination.save()\n</code></pre> <p>Strategy quick links</p> <ul> <li>Schema</li> <li>Fields</li> </ul>"},{"location":"quickstart/#derive-a-source-schema-from-data","title":"Derive a source schema from data","text":"<p>Assume we have multiple data sources with a variety of formats. One could be this:</p> <p></p> <p>We import it from <code>DATASOURCE_PATH</code>, define its <code>MIMETYPE</code>, and derive a schema:</p> <pre><code>import whyqd as qd\n\ndatasource = qd.DataSourceDefinition()\ndatasource.derive_model(source=DATASOURCE_PATH, mimetype=MIMETYPE)\nschema_source = qd.SchemaDefinition()\nschema_source.derive_model(data=datasource.get)\nschema_source.fields.set_categories(name=CATEGORY_FIELD, \n                                    terms=datasource.get_data())\nschema_source.save()\n</code></pre> <p>Where the <code>CATEGORY_FIELD</code> is a <code>string</code> which identifies which data model column you want to get categorical terms from. This will identify all the unique terms in that table column and assign them as categorical terms to the field.</p> <p>Info</p> <p>whyqd supports any of the following file mime types:</p> <ul> <li><code>CSV</code>: \"text/csv\"</li> <li><code>XLS</code>: \"application/vnd.ms-excel\"</li> <li><code>XLSX</code>: \"application/vnd.openxmlformats-officedocument.spreadsheetml.sheet\"</li> <li><code>PARQUET</code> (or <code>PRQ</code>): \"application/vnd.apache.parquet\"</li> <li><code>FEATHER</code> (or <code>FTR</code>): \"application/vnd.apache.feather\"</li> </ul> <p>Specify the mime type as a text string, uppper- or lower-case. Neither of Parquet or Feather yet have official mimetypes, so this is what we're using for now.</p> <p>Strategy quick links</p> <ul> <li>Datasource</li> </ul>"},{"location":"quickstart/#crosswalk-scripting-language","title":"Crosswalk scripting language","text":"<p>Your source- and destination schemas allow you to work rapidly, without reference to the underlying data.</p> <p>All scripts are written as a text string conforming to a standardised template:</p> <p>Script template</p> <pre><code>\"ACTION &gt; 'destination_field'::'destination_term' &lt; 'source_term'::['source_field', 'source_field']\"\n</code></pre> <p>Less formally: \"Perform this action to create this destination field from these source fields.\"</p> <p>Actions use similar naming conventions as for R's Tidyr. Each  has definitions and examples you can review:</p> Action <code>&gt;</code> Field <code>&gt;</code> Term <code>&lt;</code> Term <code>&lt;</code> Field <code>&lt;</code> Rows CALCULATE X [m X,] CATEGORISE X X [X,] X DEBLANK DEDUPE DELETE_ROWS [X,] NEW [X] PIVOT_CATEGORIES X X [X,] PIVOT_LONGER [X, X] [X,] RENAME X [X] SELECT X [X,] SELECT_NEWEST X [X m X,] X SELECT_OLDEST X [X m X,] SEPARATE [X,] X [X] UNITE X X [X,] <p>Here:</p> <ul> <li><code>X</code> requires only a single term,</li> <li><code>[X]</code> only a single term, but inside square brackets,</li> <li><code>[X, X]</code> only two terms accepted,</li> <li><code>[X,]</code> accepts any number of terms,</li> <li><code>[m X,]</code> any number of terms, but each term requires a modifier,</li> <li><code>[X m X,]</code> any number of terms, but indicates a relationship between two terms defined by a modifier.</li> </ul> <p>For your example, we define the crosswalk as:</p> <pre><code>SCRIPTS = [\n  \"NEW &gt; 'la_code' &lt; ['E06000044']\",\n  \"RENAME &gt; 'ba_ref' &lt; ['Property Reference Number']\",\n  \"RENAME &gt; 'prop_ba_rates' &lt; ['Current Rateable Value']\",\n  \"RENAME &gt; 'occupant_name' &lt; ['Primary Liable party name']\",\n  \"RENAME &gt; 'postcode' &lt; ['Full Property Address']\",\n  \"CATEGORISE &gt; 'occupation_state'::False &lt; 'Current Relief Type'::['Empty Property Rate Non-Industrial', 'Empty Property Rate Industrial', 'Empty Property Rate Charitable']\",\n  \"CATEGORISE &gt; 'occupation_state_reliefs'::'small_business' &lt; 'Current Relief Type'::['Small Business Relief England', 'Sbre Extension For 12 Months', 'Supporting Small Business Relief']\",\n  \"CATEGORISE &gt; 'occupation_state_reliefs'::'vacancy' &lt; 'Current Relief Type'::['Empty Property Rate Non-Industrial', 'Empty Property Rate Industrial', 'Empty Property Rate Charitable']\",\n  \"CATEGORISE &gt; 'occupation_state_reliefs'::'retail' &lt; 'Current Relief Type'::['Retail Discount']\",\n  \"CATEGORISE &gt; 'occupation_state_reliefs'::'other' &lt; 'Current Relief Type'::['Sports Club (Registered CASC)', 'Mandatory']\",\n  \"SELECT_NEWEST &gt; 'occupation_state_date' &lt; ['Current Relief Award Start Date' + 'Current Relief Award Start Date', 'Account Start date' + 'Account Start date']\",\n]\n</code></pre> <p>We define the crosswalk as:</p> <pre><code>import whyqd as qd\n\ncrosswalk = qd.CrosswalkDefinition()\ncrosswalk.set(schema_source=SCHEMA_SOURCE, schema_destination=SCHEMA_DESTINATION)\ncrosswalk.actions.add_multi(terms=SCRIPTS)\ncrosswalk.save()\n</code></pre> <p>Strategy quick links</p> <ul> <li>Crosswalk</li> </ul>"},{"location":"quickstart/#transforms-and-validations","title":"Transforms and validations","text":"<p>Performing the work to produce validated output data is only a few lines of code:</p> <pre><code>import whyqd as qd\n\n# Transform a data source\ntransform = qd.TransformDefinition(crosswalk=crosswalk, data_source=DATA_SOURCE)\ntransform.process()\ntransform.save(directory=DIRECTORY)\n# Validate a data source\nvaliform = qd.TransformDefinition()\nvaliform.validate(\n    transform=TRANSFORM, data_destination=DESTINATION_DATA, mimetype_destination=DESTINATION_MIMETYPE\n)\n</code></pre>"},{"location":"quickstart/#next-steps","title":"Next steps","text":"<p>The code itself is relatively trivial. Where you need to spend time is in internalising the techniques you need to  write concise crosswalk scripts.</p> <p>You can continue from here with learning curation strategies and then reviewing the APIs.</p>"},{"location":"actions/calculate/","title":"CALCULATE","text":""},{"location":"actions/calculate/#whyqd.crosswalk.actions.calculate.Action","title":"<code>Action</code>","text":"<p>         Bases: <code>BaseSchemaAction</code></p> <p>Calculate the value of a field derived from the values of other fields. Requires a <code>MODIFIER</code> indicating whether the fields should be added or subtracted from the current total.</p> <p>Script template</p> <pre><code>\"CALCULATE &gt; 'destination_field' &lt; [modifier 'source_field', modifier 'source_field', etc]\"\n</code></pre> <p>Where the <code>modifier</code> is either of <code>+</code> or <code>-</code>. If you want to change the sign of the values in a column, before further arithmatic, you'll need to do this sort of script:</p> <pre><code>\"CALCULATE &gt; 'destination_field' &lt; [modifier 'source_field', modifier CALCULATE &lt; [- 'source_field']\"\n</code></pre> <p>Example</p> <pre><code>\"CALCULATE &gt; 'total' &lt; [+ 'income', - 'expenses']\"\n</code></pre> <p>Warning</p> <p>This is still very rudimentary. Enhancements should stick to basic <code>reverse Polish notation</code> arithmetic.</p>"},{"location":"actions/categorise/","title":"CATEGORISE","text":""},{"location":"actions/categorise/#whyqd.crosswalk.actions.categorise.Action","title":"<code>Action</code>","text":"<p>         Bases: <code>BaseCategoryAction</code></p> <p>Associate category terms in a source data field to a specific categorical term in a destination field. If there  are numerous source and destination terms, then individual scripts are needed for each.</p> <p>Script template</p> <pre><code>\"CATEGORISE &gt; 'destination_field'::'destination_term' &lt; 'source_field'::['source_term']\"\n</code></pre> <ul> <li>If <code>destination_term</code> is a boolean (<code>True</code> or <code>False</code>) then <code>source_term</code> hits will assign that boolean value.</li> <li>If <code>source_term</code> is a boolean (<code>True</code> or <code>False</code>) then any value in that column will be assigned <code>True</code> or    <code>False</code> and convey the <code>destination_term</code> value.    If the <code>source_term</code> is boolean, then there must only be one source term.</li> </ul> <p>Example</p> <pre><code>\"CATEGORISE &gt; 'occupation_state_reliefs'::'other' &lt; 'Current Relief Type'::['Sports Club (Registered CASC)', 'Mandatory']\"\n</code></pre>"},{"location":"actions/deblank/","title":"DEBLANK","text":""},{"location":"actions/deblank/#whyqd.crosswalk.actions.deblank.Action","title":"<code>Action</code>","text":"<p>         Bases: <code>BaseMorphAction</code></p> <p>Remove all blank columns and rows from a DataFrame.</p> <p>Script template</p> <pre><code>\"DEBLANK\"\n</code></pre>"},{"location":"actions/dedupe/","title":"DEDUPE","text":""},{"location":"actions/dedupe/#whyqd.crosswalk.actions.dedupe.Action","title":"<code>Action</code>","text":"<p>         Bases: <code>BaseMorphAction</code></p> <p>Remove all duplicated rows from a DataFrame.</p> <p>Script template</p> <pre><code>\"DEDUPE\"\n</code></pre>"},{"location":"actions/delete_rows/","title":"DELETE_ROWS","text":""},{"location":"actions/delete_rows/#whyqd.crosswalk.actions.delete_rows.Action","title":"<code>Action</code>","text":"<p>         Bases: <code>BaseMorphAction</code></p> <p>Delete rows provided in a list. They don't have to be contiguous.</p> <p>Script template</p> <pre><code>\"DELETE_ROWS &lt; [int, int, int, etc.]\"\n</code></pre> <p>Where <code>int</code> are  specific integer row references / indices. Delete specifically does not reindex the rows so that future reference calls will reference a static index.</p> <p>Example</p> <pre><code>\"DELETE_ROWS &lt; [0, 1, 2, 3]\"\n</code></pre>"},{"location":"actions/new/","title":"NEW","text":""},{"location":"actions/new/#whyqd.crosswalk.actions.new.Action","title":"<code>Action</code>","text":"<p>         Bases: <code>BaseSchemaAction</code></p> <p>Add a new field to the dataframe, populated with a single value.</p> <p>Script template</p> <pre><code>\"NEW &gt; 'destination_field' &lt; ['value']\"\n</code></pre> <p>Where <code>value</code> is a unique <code>string</code> value which will be assigned to the entire column. This is useful where data form part of a series and each transformed dataset will be concatenated into a single larger table. This can function as a grouping identifier.</p> <p>NOTE: the <code>value</code> will be assigned as the default term for the <code>destination_field</code> FieldModel is part of the Schema, and the <code>default</code> in the <code>constraints</code>. Useful when creating a data series column. Will usually be added automatically after parsing of the scripts.</p> <p>Example</p> <pre><code>\"NEW &gt; 'la_code' &lt; ['E06000044']\"\n</code></pre>"},{"location":"actions/pivot_categories/","title":"PIVOT_CATEGORIES","text":""},{"location":"actions/pivot_categories/#whyqd.crosswalk.actions.pivot_categories.Action","title":"<code>Action</code>","text":"<p>         Bases: <code>BaseCategoryAction</code></p> <p>Convert row-level categories into field categorisations.</p> <p>Script template</p> <pre><code>\"PIVOT_CATEGORIES &gt; 'destination_field' &lt; 'source_field'::[int, int, int, etc.]\"\n</code></pre> <p>Where <code>int</code> contains the rows that define the categories, and <code>field</code> are the fields to include. Makes several assumptions:</p> <ul> <li>Rows may contain more than one category</li> <li>All terms in indexed rows in the same field are related</li> <li>Categories are assigned downwards to all rows between indices</li> <li>The last indexed category is assigned to all rows to the end of the table</li> </ul> <p>Example</p> <pre><code>\"PIVOT_CATEGORIES &gt; 'NewField' &lt; 'Field'::[1, 5]\"\n</code></pre> <p>Will transform:</p> ID Field 1 Cat1 2 Term2 3 Term3 4 Term4 5 Cat2 6 Term6 7 Term7 8 Term8 <p>To:</p> ID Field NewField 2 Term2 Cat1 3 Term3 Cat1 4 Term4 Cat1 6 Term6 Cat2 7 Term7 Cat2 8 Term8 Cat2"},{"location":"actions/pivot_longer/","title":"PIVOT_LONGER","text":""},{"location":"actions/pivot_longer/#whyqd.crosswalk.actions.pivot_longer.Action","title":"<code>Action</code>","text":"<p>         Bases: <code>BaseSchemaAction</code></p> <p>Pivot a list of columns to create a new long table format.</p> <p>Script template</p> <pre><code>\"PIVOT_LONGER &gt; ['name_field', 'value_field'] &lt; ['source_field', 'source_field', etc.]\"\n</code></pre> <p>Will assign to the destination fields as:</p> <ul> <li>name_field: containing the original source field names, as appropriate,</li> <li>value_field: containing the original values corresponding to the original source fields.</li> </ul> <p>Example</p> <pre><code>\"PIVOT_LONGER &gt; ['year', 'values'] &lt; ['1990', '1991', '1992', '1993']\"\n</code></pre>"},{"location":"actions/rename/","title":"RENAME","text":""},{"location":"actions/rename/#whyqd.crosswalk.actions.rename.Action","title":"<code>Action</code>","text":"<p>         Bases: <code>BaseSchemaAction</code></p> <p>Rename the source field to the destination field.</p> <p>Script template</p> <pre><code>\"RENAME &gt; 'destination_field' &lt; ['source_field']\"\n</code></pre> <p>Where only the first <code>source_field</code> will be used.</p> <p>This - ideally - is the most commonly-used script action as many source and destination fields have the same intention but different names.</p> <p>Example</p> <pre><code>\"RENAME &gt; 'occupant_name' &lt; ['Primary Liable party name']\"\n</code></pre>"},{"location":"actions/select/","title":"SELECT","text":""},{"location":"actions/select/#whyqd.crosswalk.actions.select.Action","title":"<code>Action</code>","text":"<p>         Bases: <code>BaseSchemaAction</code></p> <p>Use sparse data from a list of fields to populate a new field by iterating over a list of fields and selecting the next value in the list.</p> <p>Script template</p> <pre><code>\"SELECT &gt; 'destination_field' &lt; ['source_field', 'source_field', etc.]\"\n</code></pre> <p>Where order of <code>source_field</code> is important, each successive field in the list has priority over the ones before it (e.g. for columns A, B &amp; C, values in C will have precedence over values in B and A). If there are nulls in A and B, but not B, then the returned value will be from B. If, however, there are values in A, B and C, then the returned value will be from C.</p> <p>Example</p> <pre><code>\"SELECT &gt; 'occupation_state_date' &lt; ['Account Start date', 'Current Relief Award Start Date']\"\n</code></pre> <p>In this example, any dates in <code>Current Relief Award Start Date</code> will have precedence over <code>Account Start date</code> while nulls will be ignored, ensuring that <code>Account Start date</code> will be returned.</p>"},{"location":"actions/select_newest/","title":"SELECT_NEWEST","text":""},{"location":"actions/select_newest/#whyqd.crosswalk.actions.select_newest.Action","title":"<code>Action</code>","text":"<p>         Bases: <code>BaseSchemaAction</code></p> <p>Create a new field by iterating over a list of fields and selecting the newest value in the list.</p> <p>Script template</p> <pre><code>\"SELECT_NEWEST &gt; 'destination_field' &lt; ['source_field' + 'source_field_date', 'source_field' + 'source_field_date', etc.]\"\n</code></pre> <p>Where <code>+</code> links two columns together, explicitly declaring that the date in <code>source_field_date</code> is used to assign the order to the values in <code>source_field</code>. Unlike the <code>SELECT</code> action, <code>SELECT_NEWEST</code> takes its ordering from this date assignment.</p> <p>Example</p> <pre><code>\"SELECT_NEWEST &gt; 'occupation_state_date' &lt; ['Current Relief Award Start Date' + 'Current Relief Award Start Date', 'Account Start date' + 'Account Start date']\"\n</code></pre>"},{"location":"actions/select_oldest/","title":"SELECT_OLDEST","text":""},{"location":"actions/select_oldest/#whyqd.crosswalk.actions.select_oldest.Action","title":"<code>Action</code>","text":"<p>         Bases: <code>BaseSchemaAction</code></p> <p>Create a new field by iterating over a list of fields and selecting the oldest value in the list.</p> <p>Script template</p> <pre><code>\"SELECT_OLDEST &gt; 'destination_field' &lt; ['source_field' + 'source_field_date', 'source_field' + 'source_field_date', etc.]\"\n</code></pre> <p>Where <code>+</code> links two columns together, explicitly declaring that the date in <code>source_field_date</code> is used to assign the order to the values in <code>source_field</code>. Unlike the <code>SELECT</code> action, <code>SELECT_OLDEST</code> takes its ordering from this date assignment.</p> <p>Example</p> <pre><code>\"SELECT_OLDEST &gt; 'occupation_state_date' &lt; ['Current Relief Award Start Date' + 'Current Relief Award Start Date', 'Account Start date' + 'Account Start date']\"\n</code></pre>"},{"location":"actions/separate/","title":"SEPARATE","text":""},{"location":"actions/separate/#whyqd.crosswalk.actions.separate.Action","title":"<code>Action</code>","text":"<p>         Bases: <code>BaseMorphAction</code></p> <p>Separate the string values in a single column into any number of new columns on a specified key.</p> <p>Script template</p> <pre><code>\"SEPARATE &gt; ['destination_field_1', 'destination_field_2', etc. ] &lt; 'by'::['source_field']\"\n</code></pre> <p>Where <code>'by'</code> can be any <code>string</code> with one MAJOR caveat. Since <code>::</code> is used in the query, you cannot use it for separating.</p> <p>Example</p> <pre><code>\"SEPARATE &gt; ['new_1', 'new_2', 'new_3', 'new_4'] &lt; ';;'::['separate_column']\"\n</code></pre> <p>Will transform:</p> ID separate_column 2 Dogs;;Cats;;Fish 3 Cats;;Bats;;Hats;;Mats 4 Sharks;;Crabs <p>Bitwise into each destination field. If more fields are required than are assigned, will raise an error.</p> ID separate_column new_1 new_2 new_3 new_4 2 Dogs;;Cats;;Fish Dogs Cats Fish 3 Cats;;Bats;;Hats;;Mats Cats Bats Hats Mats 4 Sharks;;Crabs Sharks Crabs <p>You will need a different strategy if separation results in more categories than you have destination fields for.</p>"},{"location":"actions/unite/","title":"UNITE","text":""},{"location":"actions/unite/#whyqd.crosswalk.actions.unite.Action","title":"<code>Action</code>","text":"<p>         Bases: <code>BaseMorphAction</code></p> <p>Unite a list of columns with a selected <code>string</code> (i.e. concatenating text in multiple fields).</p> <p>Script template</p> <pre><code>\"UNITE &gt; 'destination_field' &lt; 'by'::['source_field', 'source_field', etc.]\"\n</code></pre> <p>Or: <pre><code>\"UNITE &gt; 'destination_field' &lt; ['source_field', 'source_field', etc.]\"\n</code></pre></p> <p>Where <code>'by'</code> can be any <code>string</code> with one MAJOR caveat. Since <code>::</code> is used in the query, you cannot use it for joining. By default this will be <code>', '</code> and so, alternatively, you can leave the <code>'by'</code> term out.</p> <p>Example</p> <pre><code>\"UNITE &gt; 'reference' &lt; ', '::['Reference 1', 'Reference 2', 'Reference 3']\"\n</code></pre>"},{"location":"api/action/","title":"CRUDAction","text":""},{"location":"api/action/#whyqd.crud.action.CRUDAction","title":"<code>CRUDAction</code>","text":"<p>         Bases: <code>CRUDBase[ActionScriptModel]</code></p> <p>Create, Read, Update and Delete Field Models. Usually instantiated as part of a CrosswalkDefinition and accessed as <code>.actions</code>.</p> <p>Base CRUD operations are common for both <code>CRUDField</code> and <code>CRUDAction</code>.</p> Example <pre><code>import whyqd as qd\n\ncrosswalk = qd.CrosswalkDefinition()\ncrosswalk.set(schema_source=SCHEMA_SOURCE, schema_destination=SCHEMA_DESTINATION)\n# Create the crosswalk\nschema_scripts = [\n    \"DEBLANK\",\n    \"DEDUPE\",\n    \"DELETE_ROWS &lt; [0, 1, 2, 3]\",\n    f\"PIVOT_LONGER &gt; ['year', 'values'] &lt; {datasource.model[0].names[4:]}\",\n    \"RENAME &gt; 'indicator_code' &lt; ['Indicator Code']\",\n    \"RENAME &gt; 'indicator_name' &lt; ['Indicator Name']\",\n    \"RENAME &gt; 'country_code' &lt; ['Country Code']\",\n    \"RENAME &gt; 'country_name' &lt; ['Country Name']\",\n]\ncrosswalk.actions.add_multi(terms=schema_scripts)\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>schema_source</code> <code>SchemaDefinition</code> <p>A SchemaDefinition for access to source schema definitions and operations.</p> <code>None</code> <code>schema_destination</code> <code>SchemaDefinition</code> <p>A SchemaDefinition for access to destination schema definitions and operations.</p> <code>None</code>"},{"location":"api/action/#whyqd.crud.action.CRUDAction.add","title":"<code>add(*, term)</code>","text":"<p>Add the string term for an action script. Validate as well. Does not test for uniqueness.</p> <p>Parameters:</p> Name Type Description Default <code>term</code> <code>str | ActionScriptModel</code> <p>A string term, or ActionScriptModel conforming to the script structure for a specific action.</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>If script is invalid. In most cases, you should get reasonable feedback from the error message           as to how to fix the problem.</p>"},{"location":"api/action/#whyqd.crud.action.CRUDAction.get","title":"<code>get(*, name)</code>","text":"<p>Get a specific script from the list of scripts, called by a unique id.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str | UUID</code> <p>Scripts may be duplicated, but UUIDs will be unique.</p> required <p>Returns:</p> Type Description <code>ActionScriptModel | None</code> <p>An ActionScriptModel or None, of no such script is found.</p>"},{"location":"api/action/#whyqd.crud.action.CRUDAction.get_action","title":"<code>get_action(*, script)</code>","text":"<p>Return the first action term from a script as its Model type.</p> <p>Parameters:</p> Name Type Description Default <code>script</code> <code>str</code> <p>A string term conforming to the script structure for a specific action.</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>If not a valid ACTION.</p> <p>Returns:</p> Type Description <code>BaseSchemaAction | BaseMorphAction | BaseCategoryAction</code> <p>The action model type conforming to the requirements for a script.</p>"},{"location":"api/action/#whyqd.crud.action.CRUDAction.get_action_parser","title":"<code>get_action_parser(*, script, action=None)</code>","text":"<p>Return the ACTION parser for a script.</p> <p>Parameters:</p> Name Type Description Default <code>script</code> <code>str</code> <p>A string term conforming to the script structure for a specific action.</p> required <code>action</code> <code>BaseSchemaAction | BaseMorphAction | BaseCategoryAction</code> <p>The action model type conforming to the requirements for a script.</p> <code>None</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the script can't be parsed.</p> <p>Returns:</p> Type Description <code>ActionParser | MorphParser | CategoryParser</code> <p>Parser for Action type, and used in TransformDefinition and CrosswalkDefinition.</p>"},{"location":"api/action/#whyqd.crud.action.CRUDAction.parse","title":"<code>parse(*, script)</code>","text":"<p>Parse a script for any action, of any type, and return the corresponding transformation dictionary structure. Can also be used as a validation step.</p> <p>Parameters:</p> Name Type Description Default <code>script</code> <code>str | ActionScriptModel</code> <p>A string term, or ActionScriptModel conforming to the script structure for a specific action.</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>If the script term is not valid, or if the schemas are not set.</p> <p>Returns:</p> Type Description <code>dict</code> <pre><code># A dictionary of the appropriate form for the ACTION\n{\n    \"action\": BaseSchemaAction,\n    \"destination\": FieldModel,\n    \"source\": list[ModifierModel | FieldModel]\n}\n\n{\n    \"action\": BaseMorphAction,\n    \"destination\": FieldModel | list[FieldModel],\n    \"source\": list[FieldModel],\n    \"rows\": list[int],\n    \"source_param\": str,\n    \"destination_param\": str\n}\n\n{\n    \"action\": BaseCategoryAction,\n    \"destination\": FieldModel,\n    \"category\": CategoryModel,\n    \"source\": list[FieldModel],\n    \"assigned\": list[CategoryModel],\n    \"unassigned\": list[CategoryModel],\n}\n</code></pre>"},{"location":"api/action/#whyqd.crud.action.CRUDAction.remove","title":"<code>remove(*, name)</code>","text":"<p>Remove a specific term, called by a unique <code>UUID</code>.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str | UUID</code> <p>Unique model UUID for the script.</p> required"},{"location":"api/action/#whyqd.crud.action.CRUDAction.set_schema","title":"<code>set_schema(*, schema_source=None, schema_destination=None)</code>","text":"<p>Set SchemaDefinitions.</p> <p>Parameters:</p> Name Type Description Default <code>schema_source</code> <code>SchemaDefinition</code> <p>A SchemaDefinition for access to source schema definitions and operations.</p> <code>None</code> <code>schema_destination</code> <code>SchemaDefinition</code> <p>A SchemaDefinition for access to destination schema definitions and operations.</p> <code>None</code>"},{"location":"api/action/#whyqd.crud.action.CRUDAction.transform","title":"<code>transform(*, df, script)</code>","text":"<p>Return a transformed dataframe according to a single script.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>pd.DataFrame</code> <p>A Pandas (Modin) dataframe.</p> required <code>script</code> <code>str | ActionScriptModel</code> <p>A string term, or ActionScriptModel conforming to the script structure for a specific action.</p> required <p>Returns:</p> Type Description <code>pd.DataFrame</code> <p>Transformed dataframe.</p>"},{"location":"api/action/#whyqd.crud.action.CRUDAction.transform_all","title":"<code>transform_all(*, df)</code>","text":"<p>Return a transformed dataframe after processing all scripts.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>pd.DataFrame</code> <p>A Pandas (Modin) dataframe.</p> required <p>Returns:</p> Type Description <code>pd.DataFrame</code> <p>Transformed dataframe.</p>"},{"location":"api/action/#whyqd.crud.action.CRUDAction.update","title":"<code>update(*, term)</code>","text":"<p>Update the parameters for a specific term, called by a unique <code>UUID</code>. If the <code>UUID</code> does not exist, then this will raise a <code>ValueError</code>.</p> <p>Parameters:</p> Name Type Description Default <code>term</code> <code>ActionScriptModel | dict</code> <p>A dictionary conforming to the ActionScriptModel.</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>If the term does not exist.</p>"},{"location":"api/action/#whyqd.crud.action.CRUDAction.validate","title":"<code>validate(*, required=False)</code>","text":"<p>Return the list of destination schema fields which are still to be crosswalked.</p> <p>Parameters:</p> Name Type Description Default <code>required</code> <code>bool</code> <p>Limit the unreconciled crosswalk fields to only those which are required.</p> <code>False</code> <p>Returns:</p> Type Description <code>list[FieldModel]</code> <p>List of FieldModels which are required but are still undefined by an ActionScriptModel.</p>"},{"location":"api/base/","title":"BaseDefinition","text":""},{"location":"api/base/#whyqd.core.base.BaseDefinition","title":"<code>BaseDefinition</code>","text":"<p>Core shared base definition functionality.</p>"},{"location":"api/base/#whyqd.core.base.BaseDefinition.describe","title":"<code>describe: dict[str, None] | None</code>  <code>property</code>","text":"<p>Get the model name, title and description.</p> <ul> <li>name: Term used for filename and referencing.</li> <li>title: Human-readable term used as name.</li> <li>description: Detailed description for the model. Reference its objective and use-case.</li> </ul> <p>Returns:</p> Type Description <code>dict[str, None] | None</code> <p>A dictionary with the <code>name</code>, <code>title</code> and <code>description</code> for the <code>Definition</code>.</p>"},{"location":"api/base/#whyqd.core.base.BaseDefinition.__repr__","title":"<code>__repr__()</code>","text":"<p>Returns the string representation of the model.</p>"},{"location":"api/base/#whyqd.core.base.BaseDefinition.get_citation","title":"<code>get_citation()</code>","text":"<p>Get the citation as a dictionary.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If no citation has been declared or the build is incomplete.</p> <p>Returns:</p> Type Description <code>dict[str, str | dict[str, str]]</code> <p>A dictionary conforming to the CitationModel.</p>"},{"location":"api/base/#whyqd.core.base.BaseDefinition.get_json","title":"<code>get_json(hide_uuid=False)</code>","text":"<p>Get the json model.</p> <p>Parameters:</p> Name Type Description Default <code>hide_uuid</code> <code>bool</code> <p>Hide all UUIDs in the nested JSON output. Mostly useful for validation assertions where the only          differences between sources are the UUIDs.</p> <code>False</code> <p>Returns:</p> Type Description <code>Json | None</code> <p>Json-conforming output, or None.</p>"},{"location":"api/base/#whyqd.core.base.BaseDefinition.save","title":"<code>save(directory=None, filename=None, created_by=None, hide_uuid=False)</code>","text":"<p>Save model as a json file.</p> <p>Parameters:</p> Name Type Description Default <code>directory</code> <code>str | None</code> <p>Defaults to working directory</p> <code>None</code> <code>filename</code> <code>str | None</code> <p>Defaults to model name</p> <code>None</code> <code>created_by</code> <code>str | None</code> <p>Declare the model creator/updater</p> <code>None</code> <code>hide_uuid</code> <code>bool</code> <p>Hide all UUIDs in the nested JSON output.</p> <code>False</code> <p>Returns:</p> Type Description <code>bool</code> <p>Boolean True if saved.</p>"},{"location":"api/base/#whyqd.core.base.BaseDefinition.set_citation","title":"<code>set_citation(*, citation)</code>","text":"<p>Update or create the citation.</p> <p>whyqd is designed to support a research process and ensure citation of the incredible work done by research-based data scientists.</p> <p>A citation has the following definable fields::</p> <ul> <li>author: The name(s) of the author(s) (in the case of more than one author, separated by <code>and</code>),</li> <li>title: The title of the work,</li> <li>url: The URL field is used to store the URL of a web page or FTP download. It is a non-standard BibTeX field,</li> <li>publisher: The publisher's name,</li> <li>institution: The institution that was involved in the publishing, but not necessarily the publisher,</li> <li>doi: The doi field is used to store the digital object identifier (DOI) of a journal article, conference paper,   book chapter or book. It is a non-standard BibTeX field. It's recommended to simply use the DOI, and not a DOI link,</li> <li>month: The month of publication (or, if unpublished, the month of creation). Use three-letter abbreviation,</li> <li>year: The year of publication (or, if unpublished, the year of creation),</li> <li>licence: The terms under which the associated resource are licenced for reuse,</li> <li>note: Miscellaneous extra information.</li> </ul> <p>Parameters:</p> Name Type Description Default <code>citation</code> <code>CitationModel</code> <p>A dictionary conforming to the CitationModel.</p> required Example <pre><code>import whyqd as qd\n\ndatasource = qd.DataSourceDefinition()\ndatasource.derive_model(source=SOURCE_DATA, mimetype=MIMETYPE)\ncitation = {\n    \"author\": \"Gavin Chait\",\n    \"month\": \"feb\",\n    \"year\": 2020,\n    \"title\": \"Portsmouth City Council normalised database of commercial ratepayers\",\n    \"url\": \"https://github.com/whythawk/whyqd/tree/master/tests/data\",\n    \"licence\": \"Attribution 4.0 International (CC BY 4.0)\",\n}\ndatasource.set_citation(citation=citation)\n</code></pre>"},{"location":"api/basecrud/","title":"CRUDBase","text":""},{"location":"api/basecrud/#whyqd.crud.base.CRUDBase","title":"<code>CRUDBase</code>","text":"<p>         Bases: <code>Generic[ModelType]</code></p> <p>CRUD object with default methods to Create, Read, Update, Delete (CRUD).</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Type[ModelType]</code> <p>A Pydantic model class</p> required"},{"location":"api/basecrud/#whyqd.crud.base.CRUDBase.add","title":"<code>add(*, term)</code>","text":"<p>Add the parameters for a specific term, called by a unique <code>name</code>. If the <code>name</code> already exists, then this will raise a <code>ValueError</code>.</p> <p>Parameters:</p> Name Type Description Default <code>term</code> <code>ModelType | dict</code> <p>A dictionary conforming to the ModelType. Model names must be unique, so a valid <code>name</code> in the model list     will have no collisions.</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>If the term already exists.</p>"},{"location":"api/basecrud/#whyqd.crud.base.CRUDBase.add_multi","title":"<code>add_multi(*, terms)</code>","text":"<p>Add multiple parameters for a specific term, called by a unique <code>name</code>. If the <code>name</code> already exists, then this will raise a <code>ValueError</code>.</p> <p>Parameters:</p> Name Type Description Default <code>terms</code> <code>list[ModelType | dict]</code> <p>A list of dictionaries conforming to the ModelType.</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>If the term already exists.</p>"},{"location":"api/basecrud/#whyqd.crud.base.CRUDBase.get","title":"<code>get(*, name)</code>","text":"<p>Get a specific model from the list of models defining this schema, called by a unique <code>name</code>.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str | UUID</code> <p>Specific name or reference UUID for a field already in the Schema.</p> required <p>Returns:</p> Type Description <code>ModelType | None</code> <p>ModelType, or None if no such <code>name</code> or <code>UUID</code>.</p>"},{"location":"api/basecrud/#whyqd.crud.base.CRUDBase.get_all","title":"<code>get_all()</code>","text":"<p>Get all models from the current list of models.</p>"},{"location":"api/basecrud/#whyqd.crud.base.CRUDBase.remove","title":"<code>remove(*, name)</code>","text":"<p>Remove a specific term, called by a unique <code>name</code>.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Specific name or reference UUID for a field already in the Schema.</p> required"},{"location":"api/basecrud/#whyqd.crud.base.CRUDBase.reorder","title":"<code>reorder(*, order)</code>","text":"<p>Reorder a list of terms.</p> <p>Parameters:</p> Name Type Description Default <code>order</code> <code>list[UUID]</code> <p>list of UUID in the desired order. Use <code>.get_all()</code> to view the current list.</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>If the list of <code>UUIDs</code> doesn't conform to that in the list of terms.</p>"},{"location":"api/basecrud/#whyqd.crud.base.CRUDBase.reset","title":"<code>reset()</code>","text":"<p>Reset a list of ModelType terms to an empty list.</p>"},{"location":"api/basecrud/#whyqd.crud.base.CRUDBase.update","title":"<code>update(*, term)</code>","text":"<p>Update the parameters for a specific term, called by a unique <code>name</code>. If the <code>name</code> does not exist, then this will raise a <code>ValueError</code>.</p> <p>Parameters:</p> Name Type Description Default <code>term</code> <code>ModelType | dict</code> <p>A dictionary conforming to the ModelType.</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>If the term does not exist</p>"},{"location":"api/crosswalk/","title":"CrosswalkDefinition","text":""},{"location":"api/crosswalk/#whyqd.core.crosswalk.CrosswalkDefinition","title":"<code>CrosswalkDefinition</code>","text":"<p>         Bases: <code>BaseDefinition</code></p> <p>Create and manage a method to perform a schema to schema crosswalk.</p> <p>Strategy</p> <p>Guidance on how to use this definition is in the strategies section on Crosswalk Strategies.</p> <p>Parameters:</p> Name Type Description Default <code>crosswalk</code> <code>CrosswalkModel | dict | Path | str | None</code> <p>A dictionary conforming to the CrosswalkModel, or a path to a saved definition.</p> <code>None</code> <code>schema_source</code> <code>SchemaDefinition | SchemaModel | dict | Path | str | None</code> <p>Path to a json file containing a saved schema, or a dictionary conforming to one, or a definition.</p> <code>None</code> <code>schema_destination</code> <code>SchemaDefinition | SchemaModel | dict | Path | str | None</code> <p>Path to a json file containing a saved schema, or a dictionary conforming to one, or a definition.</p> <code>None</code> Example <p>Create a new <code>CrosswalkDefinition</code> as follows:</p> <pre><code>import whyqd as qd\n\ncrosswalk = qd.CrosswalkDefinition()\ncrosswalk.set(schema_source=SCHEMA_SOURCE, schema_destination=SCHEMA_DESTINATION)\n</code></pre>"},{"location":"api/crosswalk/#whyqd.core.crosswalk.CrosswalkDefinition.actions","title":"<code>actions: CRUDAction</code>  <code>property</code>","text":"<p>Returns the active crud model for all Action operations. See Action CRUD for API.</p> <p>Returns:</p> Type Description <code>CRUDAction</code> <p>For all Action CRUD behaviours.</p>"},{"location":"api/crosswalk/#whyqd.core.crosswalk.CrosswalkDefinition.get","title":"<code>get: CrosswalkModel | None</code>  <code>property</code>","text":"<p>Get the crosswalk model.</p> <p>Returns:</p> Type Description <code>CrosswalkModel | None</code> <p>A Pydantic CrosswalkModel or None</p>"},{"location":"api/crosswalk/#whyqd.core.crosswalk.CrosswalkDefinition.set","title":"<code>set(*, crosswalk=None, schema_source=None, schema_destination=None)</code>","text":"<p>Update or create the crosswalk.</p> <p>Parameters:</p> Name Type Description Default <code>crosswalk</code> <code>CrosswalkModel | dict | Path | str | None</code> <p>A dictionary conforming to the CrosswalkModel, or a path to a saved definition.</p> <code>None</code> <code>schema_source</code> <code>SchemaDefinition | SchemaModel | dict | Path | str | None</code> <p>Path to a json file containing a saved schema, or a dictionary conforming to one, or a definition.</p> <code>None</code> <code>schema_destination</code> <code>SchemaDefinition | SchemaModel | dict | Path | str | None</code> <p>Path to a json file containing a saved schema, or a dictionary conforming to one, or a definition.</p> <code>None</code>"},{"location":"api/crosswalk/#whyqd.core.crosswalk.CrosswalkDefinition.validate","title":"<code>validate()</code>","text":"<p>Validate that all required fields are returned from the crosswalk.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If required destination fields are not present in the crosswalk.</p> <p>Returns:</p> Type Description <code>bool</code> <p>A boolean <code>True</code> if successful.</p>"},{"location":"api/datasource/","title":"DataSourceDefinition","text":""},{"location":"api/datasource/#whyqd.core.datasource.DataSourceDefinition","title":"<code>DataSourceDefinition</code>","text":"<p>         Bases: <code>BaseDefinition</code></p> <p>Create and manage a data source schema.</p> <p>Strategy</p> <p>Guidance on how to use this definition is in the strategies section on DataSource Strategies.</p> <p>Info</p> <p>whyqd supports any of the following file mime types:</p> <ul> <li><code>CSV</code>: \"text/csv\"</li> <li><code>XLS</code>: \"application/vnd.ms-excel\"</li> <li><code>XLSX</code>: \"application/vnd.openxmlformats-officedocument.spreadsheetml.sheet\"</li> <li><code>PARQUET</code> (or <code>PRQ</code>): \"application/vnd.apache.parquet\"</li> <li><code>FEATHER</code> (or <code>FTR</code>): \"application/vnd.apache.feather\"</li> </ul> <p>Specify the mime type as a text string, uppper- or lower-case. Neither of Parquet or Feather yet have official mimetypes, so this is what we're using for now.</p> <p>Parameters:</p> Name Type Description Default <code>source</code> <code>Path | str | DataSourceModel | None</code> <p>A path to a json file containing a saved schema, or a dictionary conforming to the DataSourceModel.</p> <code>None</code> Example <p>Create and validate a new <code>DataSourceDefinition</code> as follows:</p> <pre><code>import whyqd as qd\n\ndatasource = qd.DataSourceDefinition()\ndatasource.derive_model(source=DATASOURCE_PATH, mimetype=MIMETYPE)\ndatasource.save(directory=DIRECTORY)\ndatasource.validate()\n</code></pre>"},{"location":"api/datasource/#whyqd.core.datasource.DataSourceDefinition.get","title":"<code>get: DataSourceModel | list[DataSourceModel] | None</code>  <code>property</code>","text":"<p>Get the data source model.</p> <p>Warning</p> <p>If your source data are <code>Excel</code>, and that spreadsheet consists of multiple <code>sheets</code>, then whyqd will produce multiple data models which will be returned as a list. Each model will reflect the metadata for each sheet.</p> <p>As always look at your data and test before implementing in code. You should see an additional <code>sheet_name</code> field.</p> <p>Returns:</p> Type Description <code>DataSourceModel | list[DataSourceModel] | None</code> <p>Pydantic DataSourceModel as a list, a single, or None</p>"},{"location":"api/datasource/#whyqd.core.datasource.DataSourceDefinition.derive_model","title":"<code>derive_model(*, source, mimetype, header=0, **attributes)</code>","text":"<p>Derive a data model schema (or list) from a data source. All columns will be coerced to <code>string</code> type to preserve data quality even though this is far less efficient.</p> <p>Info</p> <p>whyqd supports any of the following file mime types:</p> <ul> <li><code>CSV</code>: \"text/csv\"</li> <li><code>XLS</code>: \"application/vnd.ms-excel\"</li> <li><code>XLSX</code>: \"application/vnd.openxmlformats-officedocument.spreadsheetml.sheet\"</li> <li><code>PARQUET</code> (or <code>PRQ</code>): \"application/vnd.apache.parquet\"</li> <li><code>FEATHER</code> (or <code>FTR</code>): \"application/vnd.apache.feather\"</li> </ul> <p>Specify the mime type as a text string, uppper- or lower-case. Neither of Parquet or Feather yet have official mimetypes, so this is what we're using for now.</p> <p>Parameters:</p> Name Type Description Default <code>source</code> <code>Path | str</code> <p>Source filename.</p> required <code>mimetype</code> <code>str | MimeType</code> <p>Pandas can read a diversity of mimetypes. whyqd supports <code>xls</code>, <code>xlsx</code>, <code>csv</code>, <code>parquet</code> and <code>feather</code>.</p> required <code>header</code> <code>int | list[int | None] | None</code> <p>Row (0-indexed) to use for the column labels of the parsed DataFrame. If there are multiple sheets, then         a list of integers should be provided. If <code>header</code> is <code>None</code>, row 0 will be treated as values and a         set of field names will be generated indexed to the number of data columns.</p> <code>0</code> <code>attributes</code> <p>dict of specific <code>mimetype</code> related Pandas attributes. Use sparingly.</p> <code>{}</code> <p>Returns:</p> Type Description <code>DataSourceModel | list[DataSourceModel]</code> <p>List of DataSourceModel, or DataSourceModel</p>"},{"location":"api/datasource/#whyqd.core.datasource.DataSourceDefinition.get_citation","title":"<code>get_citation()</code>","text":"<p>Get the citation as a dictionary.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If no citation has been declared or the build is incomplete.</p> <p>Returns:</p> Type Description <code>dict[str, str | dict[str, str]]</code> <p>A dictionary conforming to the CitationModel.</p>"},{"location":"api/datasource/#whyqd.core.datasource.DataSourceDefinition.get_data","title":"<code>get_data(*, refresh=False)</code>","text":"<p>Get a Pandas (Modin) dataframe.</p> <p>Parameters:</p> Name Type Description Default <code>refresh</code> <code>bool</code> <p>Force an update of the dataframe if there have been attribute changes.</p> <code>False</code> <p>Returns:</p> Type Description <code>pd.DataFrame | None</code> <p>A dataframe, or none.</p>"},{"location":"api/datasource/#whyqd.core.datasource.DataSourceDefinition.get_json","title":"<code>get_json(hide_uuid=False)</code>","text":"<p>Get the json model.</p> <p>Parameters:</p> Name Type Description Default <code>hide_uuid</code> <code>bool</code> <p>Hide all UUIDs in the nested JSON output. Mostly useful for validation assertions where the only          differences between sources are the UUIDs.</p> <code>False</code> <p>Returns:</p> Type Description <code>Json | None</code> <p>Json-conforming output, or None.</p>"},{"location":"api/datasource/#whyqd.core.datasource.DataSourceDefinition.save","title":"<code>save(directory=None, filename=None, created_by=None, hide_uuid=False)</code>","text":"<p>Save model as a json file.</p> <p>Parameters:</p> Name Type Description Default <code>directory</code> <code>str | None</code> <p>Defaults to working directory</p> <code>None</code> <code>filename</code> <code>str | None</code> <p>Defaults to model name</p> <code>None</code> <code>created_by</code> <code>str | None</code> <p>Declare the model creator/updater</p> <code>None</code> <code>hide_uuid</code> <code>bool</code> <p>Hide all UUIDs in the nested JSON output.</p> <code>False</code> <p>Returns:</p> Type Description <code>bool</code> <p>Boolean True if saved.</p>"},{"location":"api/datasource/#whyqd.core.datasource.DataSourceDefinition.set","title":"<code>set(*, schema=None)</code>","text":"<p>Update or create the schema.</p> <p>Parameters:</p> Name Type Description Default <code>schema</code> <code>Path | str | DataSourceModel | None</code> <p>A dictionary conforming to the DataSourceModel.</p> <code>None</code>"},{"location":"api/datasource/#whyqd.core.datasource.DataSourceDefinition.set_citation","title":"<code>set_citation(*, citation, index=None)</code>","text":"<p>Update or create the citation.</p> <p>Parameters:</p> Name Type Description Default <code>citation</code> <code>CitationModel</code> <p>A dictionary conforming to the CitationModel.</p> required <code>index</code> <code>int</code> <p>If there are multiple sources from the source data, provide the index (base 0) for the resource citation.</p> <code>None</code>"},{"location":"api/datasource/#whyqd.core.datasource.DataSourceDefinition.validate","title":"<code>validate()</code>","text":"<p>Validate that all required fields are returned from the crosswalk.</p>"},{"location":"api/field/","title":"CRUDField","text":""},{"location":"api/field/#whyqd.crud.field.CRUDField","title":"<code>CRUDField</code>","text":"<p>         Bases: <code>CRUDBase[FieldModel]</code></p> <p>Create, Read, Update and Delete Field Models. Usually instantiated as part of a SchemaDefinition and accessed as <code>.fields</code>.</p> <p>Base CRUD operations are common for both <code>CRUDField</code> and <code>CRUDAction</code>.</p> Example <pre><code>import whyqd as qd\n\nschema: qd.models.SchemaModel = {\n    \"name\": \"urban_population\",\n    \"title\": \"Urban population\",\n    \"description\": \"Urban population refers to people living in urban areas as defined by national statistical offices.\",\n}\nfields: list[qd.models.FieldModel] = [\n    {\n        \"name\": \"indicator_code\",\n        \"title\": \"Indicator Code\",\n        \"type\": \"string\",\n        \"description\": \"World Bank code reference for Indicator Name.\",\n        \"constraints\": {\"required\": True},\n    },\n    {\n        \"name\": \"country_name\",\n        \"title\": \"Country Name\",\n        \"type\": \"string\",\n        \"description\": \"Official country names.\",\n        \"constraints\": {\"required\": True},\n    },\n    {\n        \"name\": \"country_code\",\n        \"title\": \"Country Code\",\n        \"type\": \"string\",\n        \"description\": \"UN ISO 3-letter country code.\",\n        \"constraints\": {\"required\": True},\n    },\n    {\n        \"name\": \"indicator_name\",\n        \"title\": \"Indicator Name\",\n        \"type\": \"string\",\n        \"description\": \"Indicator described in the data series.\",\n        \"constraints\": {\"required\": True},\n    },\n    {\n        \"name\": \"year\",\n        \"title\": \"Year\",\n        \"type\": \"year\",\n        \"description\": \"Year of release.\",\n        \"constraints\": {\"required\": True},\n    },\n    {\n        \"name\": \"values\",\n        \"title\": \"Values\",\n        \"type\": \"number\",\n        \"description\": \"Value for the Year and Indicator Name.\",\n        \"constraints\": {\"required\": True},\n    },\n]\nschema_destination = qd.SchemaDefinition()\nschema_destination.set(schema=schema)\nschema_destination.fields.add_multi(terms=fields)\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>model</code> <p>The Pydantic model on which CRUD operations are based.</p> required"},{"location":"api/field/#whyqd.crud.field.CRUDField.get_category","title":"<code>get_category(*, name, category)</code>","text":"<p>Get a specific field from the list of fields defining this schema, called by a unique <code>name</code>.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Field names must be unique, so a valid <code>name</code> in the field list will have no collisions.</p> required <code>category</code> <code>bool | str</code> <p>Category name for a destination category term from the schema.</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>If <code>FieldModel</code> has not <code>category</code> in <code>ConstraintModel</code>.</p> <p>Returns:</p> Type Description <code>CategoryModel | None</code> <p>A list of CategoryModel, or None of none are defined.</p>"},{"location":"api/field/#whyqd.crud.field.CRUDField.get_constraints","title":"<code>get_constraints(*, name)</code>","text":"<p>Get the constraint parameters for a specific field defined in this schema, called by a unique <code>name</code> already in the schema.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str | UUID</code> <p>Specific name or reference UUID for a field already in the Schema.</p> required <p>Returns:</p> Type Description <code>ConstraintsModel | None</code> <p>A ConstraintsModel or None.</p>"},{"location":"api/field/#whyqd.crud.field.CRUDField.get_required","title":"<code>get_required()</code>","text":"<p>Return a list of all required fields.</p> <p>Returns:</p> Type Description <code>list[FieldModel]</code> <p>A list of required fields.</p>"},{"location":"api/field/#whyqd.crud.field.CRUDField.set_categories","title":"<code>set_categories(*, name, terms=None, as_bool=False, has_array=False)</code>","text":"<p>Derive unique category constraints from a data column for a specific field, called by a unique <code>name</code> already in the schema.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Specific name for a field already in the Schema</p> required <code>terms</code> <code>pd.DataFrame | pd.Series | list[str] | None</code> <p>Data provided as a table or column containing string terms, as arrays or individual terms</p> <code>None</code> <code>as_bool</code> <code>bool</code> <p>Set the category terms as booleans, <code>True</code> and <code>False</code></p> <code>False</code> <code>has_array</code> <code>bool</code> <p>Set <code>True</code> if data terms are arrays, or the Field is of type <code>FieldType.ARRAY</code></p> <code>False</code>"},{"location":"api/field/#whyqd.crud.field.CRUDField.set_constraints","title":"<code>set_constraints(*, name, constraints)</code>","text":"<p>Set the constraint parameters for a specific field to define this schema, called by a unique <code>name</code> already in the schema.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str | UUID</code> <p>Specific name or reference UUID for a field already in the Schema</p> required <code>constraints</code> <code>ConstraintsModel | None</code> <p>A dictionary conforming to the ConstraintsModel, or None. If None, then constraints are deleted.</p> required"},{"location":"api/schema/","title":"SchemaDefinition","text":""},{"location":"api/schema/#whyqd.core.schema.SchemaDefinition","title":"<code>SchemaDefinition</code>","text":"<p>         Bases: <code>BaseDefinition</code></p> <p>Create and manage a metadata schema.</p> <p>Strategy</p> <p>Guidance on how to use this definition is in the strategies section on Schema Strategies.</p> <p>Parameters:</p> Name Type Description Default <code>source</code> <code>Path | str | SchemaModel | None</code> <p>A path to a json file containing a saved schema, or a dictionary conforming to the SchemaModel.</p> <code>None</code> Example <p>Create a new <code>SchemaDefinition</code> as follows:</p> <pre><code>import whyqd as qd\n\nschema: qd.models.SchemaModel = {\n    \"name\": \"urban_population\",\n    \"title\": \"Urban population\",\n    \"description\": \"Urban population refers to people living in urban areas as defined by national statistical offices.\",\n}\nschema_destination = qd.SchemaDefinition()\nschema_destination.set(schema=schema)\n</code></pre>"},{"location":"api/schema/#whyqd.core.schema.SchemaDefinition.fields","title":"<code>fields: CRUDField</code>  <code>property</code>","text":"<p>Returns the active crud model for all Field operations. See Field CRUD for API.</p> <p>Returns:</p> Type Description <code>CRUDField</code> <p>For all Field CRUD behaviours.</p>"},{"location":"api/schema/#whyqd.core.schema.SchemaDefinition.get","title":"<code>get: SchemaModel | None</code>  <code>property</code>","text":"<p>Get the schema model.</p> <p>Returns:</p> Type Description <code>SchemaModel | None</code> <p>Pydantic SchemaModel or None</p>"},{"location":"api/schema/#whyqd.core.schema.SchemaDefinition.derive_model","title":"<code>derive_model(*, data)</code>","text":"<p>Derive a schema model from a data source model.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>DataSourceModel | dict</code> <p>A model defining a single data source</p> required"},{"location":"api/schema/#whyqd.core.schema.SchemaDefinition.set","title":"<code>set(*, schema=None)</code>","text":"<p>Update or create the schema.</p> <p>Parameters:</p> Name Type Description Default <code>schema</code> <code>Path | str | SchemaModel | None</code> <p>A dictionary, or path to a dictionary or json file, conforming to the SchemaModel.</p> <code>None</code>"},{"location":"api/transform/","title":"TransformDefinition","text":""},{"location":"api/transform/#whyqd.core.transform.TransformDefinition","title":"<code>TransformDefinition</code>","text":"<p>         Bases: <code>BaseDefinition</code></p> <p>Create and manage a transform to perform a schema to schema crosswalk on a tabular data source.</p> <p>Parameters:</p> Name Type Description Default <code>transform</code> <code>TransformModel | dict | Path | str | None</code> <p>Path to a transform definition, or a dictionary conforming to a transform model.</p> <code>None</code> <code>crosswalk</code> <code>CrosswalkDefinition | CrosswalkModel | dict | Path | str | None</code> <p>A definition, or a dictionary conforming to the CrosswalkModel, or a path to a saved definition.</p> <code>None</code> <code>data_source</code> <code>DataSourceModel | dict | Path | str | None</code> <p>Path to a tabular data source, or a dictionary conforming to a data source model.</p> <code>None</code> Example <p>Create a new <code>TransformDefinition</code>, and perform a crosswalk, then save the definition and transformed data as follows:</p> <pre><code>import whyqd as qd\n\ntransform = qd.TransformDefinition(crosswalk=CROSSWALK, data_source=DATASOURCE)\ntransform.process()\ntransform.save(directory=DIRECTORY)\n</code></pre>"},{"location":"api/transform/#whyqd.core.transform.TransformDefinition.get","title":"<code>get: TransformModel | None</code>  <code>property</code>","text":"<p>Get the transform model.</p> <p>Returns:</p> Type Description <code>TransformModel | None</code> <p>Pydantic TransformModel or None</p>"},{"location":"api/transform/#whyqd.core.transform.TransformDefinition.process","title":"<code>process()</code>","text":"<p>Perform a crosswalk. You can access the dataframe after completion at <code>.data</code>, if it exists.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If there are missing required destination fields in the crosswalk.</p>"},{"location":"api/transform/#whyqd.core.transform.TransformDefinition.save","title":"<code>save(*, filename=None, mimetype=None, directory=None, created_by=None, hide_uuid=False)</code>","text":"<p>Save model as a json file, and save crosswalked destination dataframe as a chosen mimetype.</p> <p>Info</p> <p>whyqd supports any of the following file mime types:</p> <ul> <li><code>CSV</code>: \"text/csv\"</li> <li><code>XLS</code>: \"application/vnd.ms-excel\"</li> <li><code>XLSX</code>: \"application/vnd.openxmlformats-officedocument.spreadsheetml.sheet\"</li> <li><code>PARQUET</code> (or <code>PRQ</code>): \"application/vnd.apache.parquet\"</li> <li><code>FEATHER</code> (or <code>FTR</code>): \"application/vnd.apache.feather\"</li> </ul> <p>Specify the mime type as a text string, uppper- or lower-case. Neither of Parquet or Feather yet have official mimetypes, so this is what we're using for now.</p> <p>NOTE: by default, transformed data are saved as <code>PARQUET</code> as this is the most efficient.</p> <p>Parameters:</p> Name Type Description Default <code>directory</code> <code>str | Path | None</code> <p>Defaults to working directory</p> <code>None</code> <code>filename</code> <code>str | None</code> <p>Defaults to model name</p> <code>None</code> <code>mimetype</code> <code>str | None</code> <p>whyqd supports saving to CSV, XLS, XLSX, Feather and Parquet files. Defaults to Parquet.</p> <code>None</code> <code>created_by</code> <code>str | None</code> <p>Declare the model creator/updater</p> <code>None</code> <code>hide_uuid</code> <code>bool</code> <p>Hide all UUIDs in the nested JSON output.</p> <code>False</code> <p>Returns:</p> Type Description <code>bool</code> <p>Boolean True if saved.</p>"},{"location":"api/transform/#whyqd.core.transform.TransformDefinition.set","title":"<code>set(*, transform=None, crosswalk=None, data_source=None)</code>","text":"<p>Update or create the transform.</p> <p>Parameters:</p> Name Type Description Default <code>transform</code> <code>TransformModel | dict | Path | str | None</code> <p>Path to a transform definition, or a dictionary conforming to a transform model.</p> <code>None</code> <code>crosswalk</code> <code>CrosswalkDefinition | CrosswalkModel | dict | Path | str | None</code> <p>A definition, or a dictionary conforming to the CrosswalkModel, or a path to a saved definition.</p> <code>None</code> <code>data_source</code> <code>DataSourceModel | dict | Path | str | None</code> <p>Path to a tabular data source, or a dictionary conforming to a data source model.</p> <code>None</code>"},{"location":"api/transform/#whyqd.core.transform.TransformDefinition.validate","title":"<code>validate(*, transform, data_destination, mimetype_destination=None, data_source=None, mimetype_source=None)</code>","text":"<p>Validate the transformation process and all data checksums. Will perform all actions on each interim data source.</p> <p>Parameters:</p> Name Type Description Default <code>transform</code> <code>TransformModel | dict | Path | str</code> <p>Path to a transform definition, or a dictionary conforming to a transform model.</p> required <code>data_destination</code> <code>DataSourceModel | dict | Path | str</code> <p>Path to a tabular data source, or a dictionary conforming to a data source model. Destination                 data for crosswalk validation.</p> required <code>mimetype_destination</code> <code>str | MimeType | None</code> <p>whyqd supports reading from CSV, XLS, XLSX, Feather and Parquet files. Required if                     <code>data_destination</code> is not of <code>DataSourceModel</code>.</p> <code>None</code> <code>data_source</code> <code>DataSourceModel | dict | Path | str | None</code> <p>Path to a tabular data source, or a dictionary conforming to a data source model. Should be defined           in <code>transform</code>, but you may have a different version from a different location.</p> <code>None</code> <code>mimetype_source</code> <code>str | MimeType | None</code> <p>whyqd supports reading from CSV, XLS, XLSX, Feather and Parquet files. Required if               <code>data_source</code> is provided (i.e. not from the <code>transform</code>) and not of <code>DataSourceModel</code>.</p> <code>None</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If any steps fail to validate.</p> <p>Returns:</p> Type Description <code>bool</code> <p>A boolean <code>True</code> on successful validation.</p>"},{"location":"strategies/crosswalk/","title":"Define crosswalks with actions &amp; transform data","text":"<p>Definition</p> <p>Crosswalks are mappings of the relationships between fields defined in different metadata  schemas. Ideally, these are one-to-one, where a field in one has an exact match in the other. In practice, it's more complicated than that.</p> <p>whyqd (/w\u026ak\u026ad/) reduces crosswalks to a series of action scripts, each defining an individual step which must be  performed to restructure source- into a destination data.</p> <p>All scripts are written as a text string conforming to a standardised template:</p> <p>Script template</p> <pre><code>\"ACTION &gt; 'destination_field'::'destination_term' &lt; 'source_term'::['source_field', 'source_field']\"\n</code></pre> <p>Less formally: \"Perform this action to create this destination field from these source fields.\"</p> <p>Actions are a wrapper around the underlying Pandas API.</p> <p>Example</p> <pre><code>\"SEPARATE &gt; ['new_1', 'new_2', 'new_3', 'new_4'] &lt; ';;'::['separate_column']\"\n</code></pre> <p>Will transform:</p> ID separate_column 2 Dogs;;Cats;;Fish 3 Cats;;Bats;;Hats;;Mats 4 Sharks;;Crabs <p>Bitwise into each destination field.</p> ID separate_column new_1 new_2 new_3 new_4 2 Dogs;;Cats;;Fish Dogs Cats Fish 3 Cats;;Bats;;Hats;;Mats Cats Bats Hats Mats 4 Sharks;;Crabs Sharks Crabs <p>And that one-line script is a wrapper for:</p> <pre><code>separate_array = np.array([x.split(source_param) for x in df[source[0].name].array.ravel()], dtype=object)\nnum_columns = max(map(len, separate_array))\nif num_columns == 1:\n    return df\nif not isinstance(destination, list) or num_columns != len(destination):\n    raise ValueError(\n        f\"SEPARATE action needs to split {num_columns} columns by received {len(destination)} for destination.\"\n    )\nnew_columns = [c.name for c in destination]\ndf[new_columns] = df[source[0].name].str.split(source_param, expand=True)\n</code></pre> <p>API</p> <p>Review the <code>class</code> API definitions: CrosswalkDefinition, CRUDAction and  TransformDefinition.</p>"},{"location":"strategies/crosswalk/#strategy-purpose-for-crosswalk-transforms","title":"Strategy &amp; purpose for crosswalk transforms","text":"<p>Strategy</p> <p>Crosswalks must be explicit rather than implicit.</p> <p>Describe all transforms in scripts. Performing each action in sequence must transform your source data to your  destination data with no other hidden assumptions or actions required.</p> <p>whyqd will support you through consistency and predictability. It will always run scripts the same way each  time.</p> <p>Curation is about recognising contextual requirements. Looking at your source data as it is. Updating rows in a  database is far easier than adding in new columns / fields. Yet people find it easier to \"read\" data like text (so in  the direction of travel - right-to-left or left-to-right, depending on language).</p> <p>When you look at source data, do so with an eye to structure and not analysis:</p> <ul> <li>Do you have merged headers spanning multiple columns and rows?</li> <li>Are categorical terms defined as sub-header data rows instead of as independent fields?</li> <li>Are values overloaded, containing both quantitative and qualitative data (such as a term and a date)?</li> <li>Has contextual information been included as redundant rows above or below your data?</li> <li>Are your data wide or long?</li> </ul> <p>When data are particularly messy, don't try and go straight to your final schema. First transform your source into an  interim form which supports its actual structure.</p> <p></p> <p>This spreadsheet is in wide format. Creating a crosswalk to get it into a well-behaved interim form will make  pivoting into a long form much easier.</p> <p>whyqd will be doing the repetitive work. Slow down and think it through.</p>"},{"location":"strategies/crosswalk/#action-scripting-summary","title":"Action scripting summary","text":"<p>Actions use similar naming conventions as for R's Tidyr and should be self-explanatory. Each has definitions and examples you can review:</p> Action <code>&gt;</code> Field <code>&gt;</code> Term <code>&lt;</code> Term <code>&lt;</code> Field <code>&lt;</code> Rows CALCULATE X [m X,] CATEGORISE X X [X,] X DEBLANK DEDUPE DELETE_ROWS [X,] NEW [X] PIVOT_CATEGORIES X X [X,] PIVOT_LONGER [X, X] [X,] RENAME X [X] SELECT X [X,] SELECT_NEWEST X [X m X,] X SELECT_OLDEST X [X m X,] SEPARATE [X,] X [X] UNITE X X [X,] <p>Here:</p> <ul> <li><code>X</code> requires only a single term,</li> <li><code>[X]</code> only a single term, but inside square brackets,</li> <li><code>[X, X]</code> only two terms accepted,</li> <li><code>[X,]</code> accepts any number of terms,</li> <li><code>[m X,]</code> any number of terms, but each term requires a modifier,</li> <li><code>[X m X,]</code> any number of terms, but indicates a relationship between two terms defined by a modifier.</li> </ul> <p>Example</p> <p>Modifiers should be self-explanatory, and this calculation script should be easy to read:</p> <pre><code>\"CALCULATE &gt; 'total' &lt; [+ 'income', - 'expenses']\"\n</code></pre> <p>A script <code>field</code> is always a term defined in the source- or destination schema. A script <code>term</code> is context-specific. It could be used to indicate categories, or text for splitting. A script <code>modifier</code> is usually a <code>+</code> or <code>-</code> and is context- specific.</p> <p><code>row</code> terms are indexed, meaning they must be in the source table or the script will fail.</p> <p>Tutorials</p> <p>It is assumed that you're not working 'blind', that you're actually looking at your data while assigning actions -  especially row-level actions - otherwise you are going to get extremely erratic results. whyqd is built on  Pandas and these examples lean heavily on that package.</p> <p>There are three worked tutorials that guide you through three typical scenarios, and you may find them helpful:</p> <ul> <li>Aligning multiple sources of local government data to a single schema</li> <li>Pivoting wide-format data into archival long-format</li> <li>Wrangling Cthulhu data without losing your mind</li> </ul>"},{"location":"strategies/crosswalk/#continuous-integration-and-automating-crosswalks","title":"Continuous Integration and automating crosswalks","text":"<p>Crosswalks are deliberately distinct from transforms.</p> <p>A crosswalk contains only the definitions for a source- and destination schema, and the action scripts that define the relationship between them.</p> <p>This makes it easy to use a <code>.crosswalk</code> definition as part of a continuous integration process, where data are harvested automatically, assigned a crosswalk, and output data transforms imported into a database.</p>"},{"location":"strategies/crosswalk/#performing-transformations","title":"Performing transformations","text":"<p>Assume you have data that looks like this:</p> Country Name Country Code Indicator Name Indicator Code 1960 1961 1962 1963 1964 1965 1966 1967 1968 1969 1970 1971 1972 1973 1974 1975 1976 1977 1978 1979 1980 1981 1982 1983 1984 1985 1986 1987 1988 1989 1990 1991 1992 1993 1994 1995 1996 1997 1998 1999 2000 2001 2002 2003 2004 2005 2006 2007 2008 2009 2010 2011 2012 2013 2014 2015 2016 2017 2018 2019 Aruba ABW Urban population SP.URB.TOTL 27526 28141 28532 28761 28924 29082 29253 29416 29575 29738 29900 30082 30275 30470 30605 30661 30615 30495 30353 30282 30332 30560 30943 31365 31676 31762 31560 31142 30753 30720 31273 32507 34116 35953 37719 39172 40232 40970 41488 41945 42444 43048 43670 44246 44669 44889 44882 44686 44378 44053 43778 43819 44057 44348 44665 44979 45296 45616 45948 nan Afghanistan AFG Urban population SP.URB.TOTL 755836 796272 839385 885228 934135 986074 1.04119e+06 1.09927e+06 1.16136e+06 1.22827e+06 1.30095e+06 1.37946e+06 1.46329e+06 1.55104e+06 1.64087e+06 1.73093e+06 1.82161e+06 1.91208e+06 1.99758e+06 2.07094e+06 2.13637e+06 2.18149e+06 2.20897e+06 2.22507e+06 2.24132e+06 2.2679e+06 2.30581e+06 2.35734e+06 2.43955e+06 2.50291e+06 2.62855e+06 2.82817e+06 3.09339e+06 3.39171e+06 3.67709e+06 3.91625e+06 4.09384e+06 4.22082e+06 4.32158e+06 4.43476e+06 4.5878e+06 4.79005e+06 5.03116e+06 5.29338e+06 5.5635e+06 5.82429e+06 6.05502e+06 6.26375e+06 6.46484e+06 6.68073e+06 6.92776e+06 7.21252e+06 7.52859e+06 7.86507e+06 8.20488e+06 8.53561e+06 8.85286e+06 9.16484e+06 9.4771e+06 nan Angola AGO Urban population SP.URB.TOTL 569222 597288 628381 660180 691532 721552 749534 776116 804107 837758 881022 944294 1.0282e+06 1.12462e+06 1.23071e+06 1.34355e+06 1.4626e+06 1.58871e+06 1.72346e+06 1.86883e+06 2.02677e+06 2.19787e+06 2.38256e+06 2.58126e+06 2.79453e+06 3.02227e+06 3.26559e+06 3.5251e+06 3.8011e+06 4.09291e+06 4.40096e+06 4.72563e+06 5.06788e+06 5.42758e+06 5.80661e+06 6.15946e+06 6.53015e+06 6.919e+06 7.32807e+06 7.75842e+06 8.212e+06 8.68876e+06 9.19086e+06 9.72127e+06 1.02845e+07 1.08828e+07 1.14379e+07 1.20256e+07 1.26446e+07 1.32911e+07 1.39631e+07 1.46603e+07 1.53831e+07 1.61303e+07 1.69008e+07 1.76915e+07 1.85022e+07 1.93329e+07 2.01847e+07 nan Albania ALB Urban population SP.URB.TOTL 493982 513592 530766 547928 565248 582374 599300 616687 635924 656733 677801 698647 720649 742333 764166 786668 809052 832109 854618 876974 902120 927513 954645 982645 1.01124e+06 1.04013e+06 1.0685e+06 1.09835e+06 1.12772e+06 1.16716e+06 1.19722e+06 1.19891e+06 1.20949e+06 1.21988e+06 1.23022e+06 1.2404e+06 1.25052e+06 1.26041e+06 1.27021e+06 1.27985e+06 1.28939e+06 1.29858e+06 1.32722e+06 1.35485e+06 1.38183e+06 1.4073e+06 1.43089e+06 1.4524e+06 1.47339e+06 1.49526e+06 1.51952e+06 1.54693e+06 1.57579e+06 1.6035e+06 1.63012e+06 1.6545e+06 1.68025e+06 1.70634e+06 1.72897e+06 nan <p>You have other data, with other indicators and spanning different (but overlapping) years, and you want to merge all of  them into a single analytical database.</p> <p>Your workflow is:</p> <ol> <li>Define a single destination schema,</li> <li>Derive a source schema from each data source,</li> <li>Review the data structure and develop a crosswalk,</li> <li>Transform and validate your outputs.</li> </ol> <p>There is a complete tutorial, and we'll focus only on the last two steps.</p> <p>Assume the list of <code>year</code> headers is <code>HEADER_YEARS</code>. Your crosswalk scripts are:</p> <pre><code>SCRIPTS = [\n    \"DEBLANK\",\n    \"DEDUPE\",\n    \"DELETE_ROWS &lt; [0, 1, 2, 3]\",\n    f\"PIVOT_LONGER &gt; ['year', 'values'] &lt; {HEADER_YEARS}\",\n    \"RENAME &gt; 'indicator_code' &lt; ['Indicator Code']\",\n    \"RENAME &gt; 'indicator_name' &lt; ['Indicator Name']\",\n    \"RENAME &gt; 'country_code' &lt; ['Country Code']\",\n    \"RENAME &gt; 'country_name' &lt; ['Country Name']\",\n]\n</code></pre> <p>There are really only two complex actions here. This particular data source has a header row that is not zero-indexed,  which is why we need to delete redundant rows. And then we pivot the year columns into year and value fields.</p> <p>As code, assign this and perform your transformation as follows:</p> <pre><code>import whyqd as qd\n\n# Crosswalk\ncrosswalk = qd.CrosswalkDefinition()\ncrosswalk.set(schema_source=schema_source, schema_destination=schema_destination)\ncrosswalk.actions.add_multi(terms=SCRIPTS)\ncrosswalk.save(directory=DIRECTORY)\n# Transform\ntransform = qd.TransformDefinition(crosswalk=crosswalk, data_source=DATA_MODEL)\ntransform.process()\ntransform.save(directory=DIRECTORY)\n</code></pre> <p>Your output is a long form table:</p> indicator_code indicator_name country_code country_name year values SP.URB.TOTL Urban population ABW Aruba 1960 27526 SP.URB.TOTL Urban population AFG Afghanistan 1960 755836 SP.URB.TOTL Urban population AGO Angola 1960 569222 SP.URB.TOTL Urban population ALB Albania 1960 493982 <p>Your saved output is:</p> <ul> <li>A <code>.transform</code> definition file that contains your source and destination schemas, the crosswalk that links them,   as well as definitions for your source and destination data models. There is also a citation and a version history.</li> <li>A data file in <code>.parquet</code> format with columns conforming to your destination schema field types.</li> </ul> <p>You can share both and know that you are providing everything anyone would need to review, audit and rerun your crosswalk.</p>"},{"location":"strategies/crosswalk/#validating-crosswalks","title":"Validating crosswalks","text":"<p>Researchers may disagree on conclusions derived from analytical results. What they should not have cause for  disagreement on is the probity of the underlying data used to produce those analytical results.</p> <p>By sharing your <code>.transforms</code> alongside your data, you allow this:</p> <pre><code>from pathlib import Path\nimport whyqd as qd\n\nDESTINATION_DATA = DIRECTORY / \"data.parquet\"\nDESTINATION_MIMETYPE = \"parquet\"\nTRANSFORM = DIRECTORY / \"reference.transform\"\nvaliform = qd.TransformDefinition()\nvaliform.validate(\n    transform=TRANSFORM, data_destination=DESTINATION_DATA, mimetype_destination=DESTINATION_MIMETYPE\n)\n\nTrue\n</code></pre> <p>Assuming the reference source data are available online (and referenced in the <code>TRANSFORM</code> model), then this will perform the crosswalk from source, derive the destination data, and compare the checksums of the data your provided with what it should have been. If everything matches, you know that the source data does produce the destination data.</p> <p>If the source data are offline (or unreachable), simply provide it. Validation also checks that the source matches what it should have been originally.</p> <pre><code>import whyqd as qd\n\nvaliform = qd.TransformDefinition()\nvaliform.validate(\n    transform=TRANSFORM, data_destination=DESTINATION_DATA, mimetype_destination=DESTINATION_MIMETYPE,\n    data_source=SOURCE_DATA, mimetype_source=SOURCE_MIMETYPE\n)\n\nTrue\n</code></pre> <p>Validation errors are reported and try to indicate the reasons for failure.</p>"},{"location":"strategies/curation/","title":"Strategies for data curation and management","text":"<p>Definition</p> <p>Data curation includes all the processes and techniques needed for ethical and reproducable data creation,  management, transformation, and presentation for reuse.</p> <p>The role of a data scientist is to support a research team in producing\u00a0an\u00a0answer to a research question that is robust,  stands up to scrutiny, and is supported by ethical measurement data acquired during a study process. </p> <p>Ensuring we know what we know would seem to be the simplest of our data curation responsibilities.</p> <p>whyqd (/w\u026ak\u026ad/) is a curatorial toolkit intended to produce well-structured and predictable data for research analysis.</p> <p>Further learning</p> <p>I have written two Jupyter Notebook-based courses which go far deeper into data curation and data science:</p> <ul> <li>Data Wrangling and Validation is an introductory   course and starts with a basic introduction to Python. It assumes zero knowledge.</li> <li>Data as a Science consists of four modules of a data science   Masters degree course (out of what will eventually be 20). It teaches data science through peer review of clinical   trials and the development of synthetic data.</li> </ul>"},{"location":"strategies/curation/#data-transformation-strategies-for-whyqd","title":"Data transformation strategies for whyqd","text":"<p>Strategy</p> <p>Transformations are performed on schemas, not source data.</p> <p>Explicit is better than implicit. Don't leave transformation steps out.</p> <p>The objective of whyqd is not to explore data, or perform interim analysis, but to support schema-based workflows  and validations, and documented and auditable transformations. </p> <p>Your approach can follow variations on schema-to-schema or data-to-schema-to-schema strategies.</p>"},{"location":"strategies/curation/#schema-to-schema","title":"Schema-to-Schema","text":"<p>This can be thought of as a relatively abstract approach since source data are not involved until the final application  step.</p> <ol> <li>Define, or import, a source schema with fields that describe and constrain columns in source data,</li> <li>Define, or import, a destination schema,</li> <li>Specify a crosswalk to perform a schema-to-schema transformation,</li> <li>Validate the transformation crosswalk,</li> <li>Save the crosswalk for future or automated / programmatic use,</li> <li>Apply this to source data.</li> </ol>"},{"location":"strategies/curation/#data-to-schema-to-schema","title":"Data-to-Schema-to-Schema","text":"<p>Here you derive a schema from source data and go from there. There is a temptation to explore the data, but that risks  biased or destructive data strategies. Focus on definitions and transformation.</p> <ol> <li>Import a source data file,</li> <li>Apply an existing schema and test whether the source conforms to that schema,</li> <li>Alternatively, derive a minimal transformable schema from the data source,</li> <li>Specify a crosswalk to perform a schema-to-schema transformation,</li> <li>Alternatively, specify a destination schema, and define a crosswalk to transform the source to destination schema,</li> <li>Validate the transformation crosswalk,</li> <li>Apply this to the source data.</li> </ol>"},{"location":"strategies/curation/#transformations-and-the-minimum-transformable-schema","title":"Transformations and the Minimum Transformable Schema","text":"<p>No matter your approach, the transformation crosswalk can be saved and applied to multiple sources which conform to the  crosswalk. That requires any source data to be defined by a schema, and transformations applied to that schema.</p> <p>Ultimately, data are transformed, but the objective is that any data meeting a particular schema definition can be  restructured using a schema transform.</p> <p>A minimum transformable schema has the following properties:</p> <ul> <li>For every column in the data source, there is a matching field in the schema. Each column is either defined in the    data's header row, or as a separate, ordered list of header names in the schema which match the listed fields.</li> <li>A <code>string</code> is a legitimate default field type and can be converted during a transformation process.</li> <li>If categorical terms are to form part of a transformation, then these categorical terms must be defined up-front as    part of the schema, and not as part of a transformation. Once data are transformed - as part of analysis - you can    introduce new categorisations or aggregations.</li> </ul> <p>That means that no matter how brutally ugly source data may be, they can be reduced to a defined schema and transformed  from there.</p>"},{"location":"strategies/curation/#classification-and-metadata","title":"Classification and metadata","text":"<p>The creator of a dataset would best know what their data are about and should assign keywords as descriptors. These data  about data are called metadata. The term is ambiguous, as it is used for two fundamentally different concepts, both of which are defined and documented in whyqd models:</p> <ul> <li>Structural metadata\u00a0correspond to internal metadata (i.e. metadata about the structure, or fields, of database    objects such as tables, columns, keys and indexes),</li> <li>Descriptive metadata\u00a0correspond to external metadata. (i.e. metadata typically used for discovery and    identification, as information used to search and locate an object such as title, author, subject, keywords,    publisher).</li> </ul> <p>Each of these help with understanding the aboutness of what is otherwise a nebulous table of obscure values.</p> <p>Descriptive metadata permits discovery of the object. Structural metadata permits the data to be applied, interpreted,  analysed, restructured, and joined to other, similar, datasets. Metadata can permit interoperability between different  systems. An agreed-upon structure for querying the\u00a0aboutness\u00a0of a data series can permit unrelated software systems  to find and use remote data.</p> <p>Beyond metadata, there are also mechanisms for the structuring of relationships between hierarchies of keywords. These  are known as ontologies and, along with metadata, can be used to accurately define and permit discovery of data.</p> <p>Adding metadata to existing data resources can be a labour-intensive and expensive process. This may become a barrier  to implementing a comprehensive data management system.</p> <p>This presents the greatest risk for data consistency. Any format change or manipulation, or even copying a file from  one system to another, introduces the potential for data corruption. Similarly, it also increases the potential for  data - whether erroneous or not - to be accidentally released to users or the public before it is ready.</p> <p>whyqd supports data curation through providing an intuitive method for deriving and creating data schemas as a  JSON Schema-compliant file. It further supports schema-to-schema transformations using an  expressive action scripting language.</p>"},{"location":"strategies/curation/#data-resolution-ethics-and-interoperability","title":"Data resolution, ethics and interoperability","text":"<p>Ethics</p> <p>The best way to reduce ethical risks during a study is not to collect unnecessary subject-specific data. If you  don't need it to answer a research question, don't collect it in the first place.</p> <p>The NIH Clinical Centre lists seven principles for guiding the conduct of  ethical research. All of these are important, but some aspects of this methodology have implications for data reuse and  interoperability.</p> <p>Singapore's Personal Data Protection Commission\u00a0 describe the following disclosure risks:</p> <ul> <li>Identity disclosure (re-identification): permitting the identity of an individual described by a specific record.    This could arise from scenarios such as insufficient anonymisation, re-identification by linking, or pseudonym    reversal.</li> <li>Attribute disclosure: determining that an attribute described in the dataset belongs to a specific individual,    even if the individual\u2019s record cannot be distinguished.</li> <li>Inference disclosure: making an inference about an individual even if they are not in the dataset, by statistical    properties of the dataset.</li> </ul> <p>The risk is not only from one dataset, but from the way in which multiple datasets can be recombined.</p> <p>A typical example of such a risk is in the release of medical history. e.g. a dataset containing anonymised patient  records of a surgeon reveals that all his patients below the age of 30 have undergone a particular procedure. If it is  known that a specific individual is 28 years old and is a client of this surgeon, we then know that this individual has  undergone the particular procedure, even if the individual's record cannot be distinguished from others in the  anonymised dataset.</p> <p>Instead of collecting patient data at the resolution of birth date, the study may choose to redact these data through  collation by categorical age:</p> <pre><code>[\"20-40\", \"40-60\", \"60-80\"]\n</code></pre> <p>That resolves the issue and ensures that patients are at far less risk to deanonymisation, even where data are  published. However, say you wish to reuse their data, but at greater resolution:</p> <pre><code>[\"20-30\", \"30-40\", \"40-50\", \"50-60\", \"60-70\", \"70-80\"]\n</code></pre> <p>There is no date of birth, and no means of allocating individuals to these categories, so transformation is not  possible. You would need to adjust your study protocol accordingly.</p> <p>Ethics</p> <p>Schema transformations are not a cavalier solution to source data requirements. A study protocol must take  cognisance of source resolution and adjust study protocols, and study data schemas, to ensure accord.</p>"},{"location":"strategies/curation/#defining-schema-properties-with-standards","title":"Defining schema properties with standards","text":"<p>JSON Schema is a suite of tools and definitions for defining and validating structural  metadata. It is not intrinsically designed to support data interoperability, but it is a useful set of definitions -  with a growing community of resources - for information curation.</p> <p>Let's look at a simple schema according to  JSON Schema's tutorial:</p> <pre><code>{\n\"$schema\": \"https://json-schema.org/draft/2020-12/schema\",\n\"title\": \"Product\",\n\"description\": \"A product from Acme's catalog\",\n\"type\": \"object\",\n\"properties\": {\n\"productId\": {\n\"description\": \"The unique identifier for a product\",\n\"type\": \"integer\"\n}\n},\n\"required\": [ \"productId\" ]\n}\n</code></pre> <ul> <li><code>$schema</code> indicates that this schema is    written according to a specific draft of the standard and used for a variety of reasons, primarily version control.</li> <li><code>$id</code> defines a URI for the schema, and    the base URI that other URI references within the schema are resolved against.</li> <li><code>title</code> and    <code>description</code> are descriptive    annotation keywords. They do not add constraints to the data being validated. The intent of the schema is stated with    these two keywords.</li> <li><code>type</code> validation keyword the    first constraint on our JSON data and in this case it has to be a JSON Object.</li> <li><code>properties</code> validation keyword. This    contains a set of objects each defining a property within the schema:<ul> <li><code>productId</code> key.<ul> <li><code>description</code> schema annotation and <code>type</code> validation keyword.</li> </ul> </li> </ul> </li> <li>The <code>required</code> validation keyword    listing <code>productId</code>.</li> </ul> <p>This is useful for reflecting a set of products for sale on a website, but not as concise or helpful for actual tabular  data. whyqd uses a variation of JSON Schema, called Table Schema:</p> <pre><code>{\n// fields is an ordered list of field descriptors\n// one for each field (column) in the table\n\"fields\": [\n// a field-descriptor\n{\n\"name\": \"name of field (e.g. column name)\",\n\"title\": \"A nicer human readable label or title for the field\",\n\"type\": \"A string specifying the type\",\n\"format\": \"A string specifying a format\",\n\"example\": \"An example value for the field\",\n\"description\": \"A description for the field\"\n// ...\n},\n// ... more field descriptors\n],\n// (optional) specification of missing values\n\"missingValues\": [ ... ],\n// (optional) specification of the primary key\n\"primaryKey\": ...\n// (optional) specification of the foreign keys\n\"foreignKeys\": ...\n}\n</code></pre> <p>whyqd's schema definitions show you how to create, or derive, a schema to reflect source data.</p>"},{"location":"strategies/curation/#auditable-data-transformation","title":"Auditable data transformation","text":"<p>Whyqd.com supports trust in research by ensuring complete and unambiguous probity in the curation  of all source data.</p> <p>Data probity refers to the following criteria:</p> <ul> <li>Identifiable input source data,</li> <li>Transparent methods for restructuring of that source data into the data used to support research analysis,</li> <li>Accessible restructured data used to support research conclusions,</li> <li>A repeatable, auditable curation process which produces the same data.</li> </ul> <p>Researchers may disagree on conclusions derived from analytical results. What they should not have cause for  disagreement on is the probity of the underlying data used to produce those analytical results.</p> <p>The strategy guides for schemas, data sources,  crosswalks, and transforms cover the process for transforming source  data to conform to a base schema, crosswalks, and transformation and validation.</p>"},{"location":"strategies/curation/#data-structure-and-machine-readable-standards","title":"Data structure and machine-readable standards","text":"<p>There are a number of common formats for data distribution. Some of these are considered \"open\" (such as CSV, XML, text  and others) and some proprietary (SAS, STATA, SPSS, etc.). XLS and XLSX, associated with Microsoft Excel, are relatively  open formats and a number of software systems can interpret the data.</p> <p>Proprietary formats are legitimate for research publication since these are the software systems used by many  professional data users. However, since these formats are often not interoperable, the potential for data re-use is  limited unless open formats are also supported. Data dissemination in proprietary formats does not preclude  dissemination in open formats and vice versa.</p> <p>Spreadsheets and distributed data systems often lack an agreed data structure. A researcher who wishes to combine this  with other data first needs to normalise it and then decide on standardised terms to define the columns and data-types  in those columns.</p> <p></p> <p>Converting semi-structured tabular data into a typical machine-readable format results in the comma-separated-value  (CSV) file. These are tabular files with a header row which defines each of the data in the columns and rows below.</p> <p></p> <p>Ignoring any further standards compliance, CSV files can be so arranged that they are \"joined\" on a common column. For  example, a set of geospatial coordinates can be used to connect a number of similar files covering different data  series.</p>"},{"location":"strategies/curation/#requirements-for-machine-readable-tabular-data","title":"Requirements for machine-readable tabular data","text":"<p>Data.gov has a useful\u00a0Primer on Machine Readability for Online Documents and Data\u00a0 and a comment thread at Data.gov.uk offers the following guidance:</p> <ul> <li>A user can open the data file in freely-available and widely accessible software packages - this means that formats    such as CSV should be preferred over, or offered in addition to formats like Excel, and proprietary formats which can    only be opened with commercial or specialist software should be avoided.</li> <li>It is possible to process the data directly, carrying out any appropriate operations on it such as sorting columns,    filtering rows, running aggregates of values - this requires well structured data. Where possible, the meaning of the    data should not be contained in the layout.</li> <li>Common elements in the dataset are expressed in uniform ways - for example, dates are always in the same format, codes    or names are always in the same case, and numbers are expressed consistently (e.g. 1,000 or 1000 but not a mixture of    the two).</li> <li>The meaning of fields and values is clearly documented - either through clear naming of fields, or through    accompanying descriptions provided along with the data.</li> <li>Machine readability is enhanced if the dataset uses common standards where they exist - including standard identifiers    and standard field names. These might be standards like the public spending vocabulary developed for government, or    third-party standards such as KML for indicating 'points of interest'.</li> </ul>"},{"location":"strategies/curation/#long-vs-wide-data-formats-for-archival-and-presentation","title":"Long\u00a0vs\u00a0wide\u00a0data formats for archival and presentation","text":"<p>Any data series consists of numerical values (usually) described by standardised metadata terms (time, area, a specific  description, etc). There are two main ways of presenting these machine-readable data, which can be summarised as\u00a0wide\u00a0 or\u00a0long. You need to make a deliberate choice as to which format you will choose, and each has its own particular  strengths and weaknesses:</p>"},{"location":"strategies/curation/#wide-data","title":"Wide data","text":"<p>Wide data\u00a0present numerical data in multiple columns. Either as categories (e.g. each country is presented in its  own column) or by date (e.g. each annual update results in a new column). New data go across the screen from left to  right:</p> <p></p> <p>Wide data\u00a0are often used for data visualisation and processing since the data can easily be grouped into the  necessary axes for chart libraries. However, it's a difficult archival format since updating such a dataseries requires  the equivalent of creating a new field (the\u00a0year\u00a0in the fields above) and then updating every row with appropriate  information. That can be an expensive operation in a large database, and also means that writing a programmatic method  for querying your data is more challenging.</p>"},{"location":"strategies/curation/#long-data","title":"Long data","text":"<p>Long data\u00a0present numerical data in multiple rows with only one column for values. New data go down the screen  from top to bottom:</p> <p></p> <p>Long data\u00a0are best for archival and for representing the structure you will ordinarily find in a database. Each row  in a\u00a0long\u00a0dataseries represents a row in a database. Adding new information is relatively straightforward since you  only need update a single row at a time. In database terms, you'd be creating a single database entry.</p> <p>The preference is for the\u00a0long\u00a0format, and this will be the method usually recommended for release. That said,  conversion between them - as long as data are machine-readable with well-defined metadata - is straightforward.</p>"},{"location":"strategies/datasource/","title":"Derive data &amp; schema models from source data","text":"<p>whyqd (/w\u026ak\u026ad/) recognises that the world data scientists operate in is far messier than your educators may have led you to believe.</p> <p>In most cases, standardised schemas are developed post-hoc, after data have been in use for some time. This is simply practical reality. It is often impossible to know what data you need to answer a research question until after you've  started trying to answer it.</p> <p>The majority of tabular data are stored in spreadsheets on people's desktop computers. For most people, Excel is both database and dashboard visualisation software. That also means that source data are designed, foremost, for  presentation.</p> <p>Such data can have any of:</p> <ul> <li>Merged headers spanning multiple columns and rows</li> <li>Random empty rows and columns</li> <li>Categorical terms defined as spacers inside data rows instead of as fields</li> <li>Joined values containing both quantitative and qualitative data (such as a term and a date)</li> <li>Non-numeric data in numeric fields (such as the multiple ways of showing \"missing\" values)</li> </ul> <p>Instead of fighting to get people, who have other concerns and responsibilities, to adopt some ideal schema, you'll need  to study your source and try and identify all the challenges in your way.</p> <p>You need to derive a schema from data, and then write a crosswalk to get to your archival destination schema standard.</p> <p>API</p> <p>Review the <code>class</code> API definitions: DataSourceDefinition.</p>"},{"location":"strategies/datasource/#derive-a-data-model-from-source-data","title":"Derive a data model from source data","text":"<p>Assume you have source data defined as follows:</p> <ul> <li><code>DATASOURCE_PATH</code> is the complete path, or URL to a source data file,</li> <li><code>MIMETYPE</code> is the source file type, whether <code>CSV</code>, <code>XLS</code>, <code>XLSX</code>, <code>PARQUET</code> or <code>FEATHER</code>,</li> <li><code>DIRECTORY</code> and the directory where you want to store your saved output.</li> </ul> <p>Info</p> <p>whyqd supports any of the following file mime types:</p> <ul> <li><code>CSV</code>: \"text/csv\"</li> <li><code>XLS</code>: \"application/vnd.ms-excel\"</li> <li><code>XLSX</code>: \"application/vnd.openxmlformats-officedocument.spreadsheetml.sheet\"</li> <li><code>PARQUET</code> (or <code>PRQ</code>): \"application/vnd.apache.parquet\"</li> <li><code>FEATHER</code> (or <code>FTR</code>): \"application/vnd.apache.feather\"</li> </ul> <p>Specify the mime type as a text string, uppper- or lower-case. Neither of Parquet or Feather yet have official mimetypes, so this is what we're using for now.</p> <p>Deriving a data model is as simple as:</p> <pre><code>import whyqd as qd\n\ndatasource = qd.DataSourceDefinition()\ndatasource.derive_model(source=DATASOURCE_PATH, mimetype=MIMETYPE)\ndatasource.save(directory=DIRECTORY)\ndatasource.validate()\n</code></pre> <p><code>.save</code> will export a JSON-Schema compliant text file with <code>.data</code> as its filetype. This captures everything about your  source data, including a citation. If your data are accessable via a persistent url, then distributing this file ensures that all metadata definitions associated with your source data are maintained.</p> <p><code>.validate</code> will repeat the derivation and test that the checksum derived for the  model is repeatable.</p> <p>whyqd can import this file and produce schemas, crosswalks and data transformations.</p> <p>Warning</p> <p>If your source data are <code>Excel</code>, and that spreadsheet consists of multiple <code>sheets</code>, then whyqd will produce  multiple data models which will be returned as a list. Each model will reflect the metadata for each sheet.</p> <p>As always look at your data and test before implementing in code. You should see an additional <code>sheet_name</code> field.</p> <p>You can read it:</p> <pre><code>datasource.get.dict(by_alias=True, exclude_defaults=True, exclude_none=True)\n\n{'uuid': UUID('827941cb-fc89-4849-beea-5779fefb9f87'),\n 'path': DATASOURCE_PATH,\n 'mime': 'application/vnd.openxmlformats-officedocument.spreadsheetml.sheet',\n 'columns': [{'uuid': UUID('7809a86c-c888-4f68-8eb2-a53a3e90c569'),\n   'name': 'Property Reference Number'},\n  {'uuid': UUID('c4296371-4808-4e28-a04b-680c3ed0cd75'),\n   'name': 'Primary Liable party name'},\n  {'uuid': UUID('21688dd3-4b17-4420-8712-d40a85ea13f3'),\n   'name': 'Full Property Address'},\n  {'uuid': UUID('4a5ba635-6cdf-49d6-9347-8975cffbab61'),\n   'name': 'Current Relief Type'},\n  {'uuid': UUID('c4196d43-8c68-495b-bd1f-0e482485f5a2'),\n   'name': 'Account Start date'},\n  {'uuid': UUID('e5ac6524-77c4-4144-878d-5aea44c3ff22'),\n   'name': 'Current Relief Award Start Date'},\n  {'uuid': UUID('916b9f15-7638-41e4-96e6-7c1b88ea257e'),\n   'name': 'Current Rateable Value'}],\n 'preserve': ['Property Reference Number',\n  'Primary Liable party name',\n  'Full Property Address',\n  'Current Relief Type',\n  'Account Start date',\n  'Current Relief Award Start Date',\n  'Current Rateable Value'],\n 'attributes': {},\n 'checksum': 'c1a67eba4344aea0264a2375145ed0991ee7ec29574d04cdeba86e651dede21aa04a00547d121c57c16005c6bfa760b959f9cae116bcaf83dc36fcb1fddb01a4',\n 'index': 3421}\n</code></pre> <p>Strategy</p> <p>You can edit this data model, changing everything from the default generated name, to the types allocated to each  field. You should definitely update <code>description</code> and give greater depth.</p> <pre><code>datasource.get.description = \"Portsmouth ratepayer data in multiple sheets. Demonstrating create method, add date, actions and perform a merge, plus filter the final result.\"\n</code></pre> <p>But, as far as the data <code>types</code> for each of the columns, I'm going to suggest that you leave this to your schema  definition.</p> <p>Machine-readability come in a variety of forms. As long as your source data meet the primary requirements of  curation, you will be able to perform analysis or transformations. As for coercing your data types? Well, your schema will do that for you.</p> <p>This derivation assumed a clear header row indexed at <code>0</code>. Unfortunately, not all source data are so easy.</p>"},{"location":"strategies/datasource/#derive-a-minimum-transformable-schema-from-ugly-data","title":"Derive a minimum transformable schema from ugly data","text":"<p>Sometimes your source data are wild.</p> <p></p> <p>UNDP Human Development Index 2007-2008: a beautiful example of messy data.</p> <p>The 2007-8 HDI report was listed as a series of about 50 spreadsheets, each dataset aligned with the objectives of the  Millennium Development Goals. These supporting information were used to track  countries meeting the MDG targets. Analysis required rebuilding these spreadsheets into a single database aligned to a  common schema.</p> <p>The temptation is to dive straight in and start physically restructuring this until you have something that meets the  minimum criteria for machine readability. That is unnecessary, and commits the cardinal sin of data curation.</p> <p>Ethics</p> <p>Never perform destructive changes to source data.</p> <p>A destructive change is one which is undocumented, and - therefore - unreproducible by others.</p> <p>Instead derive a minimum transformable schema as follows:</p> <pre><code>import whyqd as qd\n\ndatasource = qd.DataSourceDefinition()\ndatasource.derive_model(source=SOURCE_DATA, mimetype=MIMETYPE, header=None)\n</code></pre> <p>Row (0-indexed) to use for the column labels of the parsed DataFrame. If there are multiple sheets, then a list of  integers should be provided. If <code>header</code> is <code>None</code>, row 0 will be treated as values and a set of field names will be  generated indexed to the number of data columns.</p> <pre><code>datasource.data.columns\n\n    Index(['column_0', 'column_1', 'column_2', 'column_3', 'column_4', 'column_5',\n           'column_6', 'column_7', 'column_8', 'column_9', 'column_10',\n           'column_11', 'column_12', 'column_13', 'column_14', 'column_15',\n           'column_16', 'column_17', 'column_18', 'column_19', 'column_20',\n           'column_21'],\n          dtype='object')\n</code></pre> <p>Strategy</p> <p>The emphasis is on minimum transformable. Your schema is going to need to validate. <code>string</code> field types will  preserve underlying data where a column contains a mix of types. The process of transformation between schemas  permits you to correct problems and restructure your source data. Transformation gets you to where you need to be to  perform more complex data restructuring and analysis.</p> <p>Further learning</p> <p>Learn more about transforming <code>Cthulhu</code> data in the full worked tutorial for this dataset.</p>"},{"location":"strategies/datasource/#derive-schema-models-from-data-models","title":"Derive schema models from data models","text":"<p>A data model is useful for ensuring that source data are preserved, but schemas are better for  defining data and ensuring validation and compliance with research objectives.</p> <p>Getting from a data model to a schema definition is straightforward:</p> <pre><code>import whyqd as qd\n\ndatasource = qd.DataSourceDefinition()\ndatasource.derive_model(source=DATASOURCE_PATH, mimetype=MIMETYPE)\nschema_source = qd.SchemaDefinition()\nschema_source.derive_model(data=datasource.get)\n</code></pre> <p>If you review the Field operations you'll see you can now access the individual fields and apply  constraints and defaults.</p> <p>Here's how you'd derive categorical terms from a data model column:</p> <pre><code>schema_source.fields.set_categories(name=CATEGORY_FIELD, \n                                    terms=datasource.get_data())\n</code></pre> <p>Where the <code>CATEGORY_FIELD</code> is a <code>string</code> which identifies which data model column you want to get categorical terms from. This will identify all the unique terms in that table column and assign them as categorical terms to the field.</p> <p>You can also specify them <code>as_bool</code> (where terms are <code>[True, False]</code>), or you can treat terms <code>as_array</code>, where a row  may be assigned a list of categorical terms.</p> <p>An example of both of these types of categorical terms is explained in one of the worked tutorials.</p> la_code ba_ref occupant_name postcode occupation_state occupation_state_date prop_ba_rates occupation_state_reliefs E06000044 177500080710 A company PO5 2SE True 2019-04-01 98530 [small_business, retail] <p>Categorical terms are not validated on assignment, so if you choose to set arbitrary terms, that'll happen:</p> <pre><code>name = \"column_1\"\nst.set_field_categories(name=name, terms=[\"fish\", \"frog\", \"fennel\"])\nst.schema.fields.get(name=name).dict(by_alias=True, exclude_defaults=True, exclude_none=True)\n\n    {'uuid': UUID('a43b7a94-ca6f-438d-83b9-4c84d9d7a0b7'),\n     'name': 'column_1',\n     'title': 'column_1',\n     'type': 'string',\n     'constraints': {'enum': [{'uuid': UUID('f9032145-6fb9-4873-b314-1a094b79f432'),\n        'name': 'fish'},\n       {'uuid': UUID('f88c29f4-809c-4aef-b885-a8e372cba48f'), 'name': 'frog'},\n       {'uuid': UUID('ab1f0dff-c716-43df-9b00-179f211c7b04'), 'name': 'fennel'}]}}\n</code></pre> <p>Strategy</p> <p>The objective of specifying categorical terms is to support transformation, and associating categorical terms in a  source schema with those in a destination schema. You don't need to use all of them, but all terms in a source data  column must be listed in the source schema field.</p>"},{"location":"strategies/datasource/#pandas-attributes-are-available","title":"Pandas attributes are available","text":"<p>whyqd is NOT an analytical tool. It supports data curation, and so the default behaviours are to preserve source data and do as little as possible during transformation. That said, some source data - especially CSV files - are difficult to read and won't open at all without some help.</p> <p>As one example, CSV errors in opening a source file can be fixed by referencing quotation and end-of-line errors using <code>quoting=csv.QUOTE_NONE</code>. \"CSV\" is often a misnomer. Anything can be a separator, <code>sep='*'</code>.</p> <p>Arbitrary additional attributes used by Pandas for reading CSVs, Excel and other mime types supported by whyqd can  be included without limit:</p> <pre><code>import whyqd as qd\n\ndatasource = qd.DataSourceDefinition()\ndatasource.derive_model(source=DATA, mimetype=CSVTYPE, quoting=csv.QUOTE_NONE, sep=\"*\")\ndatasource.validate()\n</code></pre> <p>Use this sparingly. <code>Exceptions</code> raised as a result of any additional attributes will be Pandas exceptions.</p>"},{"location":"strategies/datasource/#schema-coersion-to-source-data","title":"Schema coersion to source data","text":"<p>Your source schema may not correspond to your input data structure immediately, and may need to be coerced. Categorical  data may need to be validated, date formats recognised, or integers converted to floats.</p> <p>whyqd will do this automatically during transformation and your output data will be coerced to their schema-defined types and constraints.</p> <p>If any data fields do not match, or require coercion to match, then you will receive a warning:</p> <pre><code>UserWarning: 1 columns in Data were coerced to appropriate dtypes in Schema. ['occupation_state_date']\nUserWarning: 2 columns in Data were coerced to appropriate dtypes in Schema. ['occupation_state', 'occupation_state_reliefs']\n</code></pre>"},{"location":"strategies/datasource/#hashing-for-data-probity","title":"Hashing for data probity","text":"<p>Those of you familiar with Dataverse's universal numerical fingerprint may be wondering where it is? whyqd, similarly, produces a unique hash for each datasource. Ours is based on  BLAKE2b and is included in the data source model.</p> <pre><code>datasource.get.checksum\n\n'a8d03afd7d5b93163dac56ba23a7c75dedf42b8999295e560f3d633d54457e9de3ae95dea8181f238e549a7fba10a36723d2f1b1d94ef3f2273129a58bfc0751'\n</code></pre> <p>This is used for validation tests to ensure that source and transformed data both match those originally used and  produced as a result of a crosswalk.</p>"},{"location":"strategies/datasource/#citation","title":"Citation","text":"<p>whyqd is designed to support a research process and ensure citation of the incredible work done by research-based data scientists.</p> <p>A citation is a special set of fields, with options for:</p> <ul> <li>author: The name(s) of the author(s) (in the case of more than one author, separated by <code>and</code>),</li> <li>title: The title of the work,</li> <li>url: The URL field is used to store the URL of a web page or FTP download. It is a non-standard BibTeX field,</li> <li>publisher: The publisher's name,</li> <li>institution: The institution that was involved in the publishing, but not necessarily the publisher,</li> <li>doi: The doi field is used to store the digital object identifier (DOI) of a journal article, conference paper,   book chapter or book. It is a non-standard BibTeX field. It's recommended to simply use the DOI, and not a DOI link,</li> <li>month: The month of publication (or, if unpublished, the month of creation). Use three-letter abbreviation,</li> <li>year: The year of publication (or, if unpublished, the year of creation),</li> <li>licence: The terms under which the associated resource are licenced for reuse,</li> <li>note: Miscellaneous extra information.</li> </ul> <p>As an example:</p> <pre><code>import whyqd as qd\n\ndatasource = qd.DataSourceDefinition()\ndatasource.derive_model(source=SOURCE_DATA, mimetype=MIMETYPE)\ncitation = {\n    \"author\": \"Gavin Chait\",\n    \"month\": \"feb\",\n    \"year\": 2020,\n    \"title\": \"Portsmouth City Council normalised database of commercial ratepayers\",\n    \"url\": \"https://github.com/whythawk/whyqd/tree/master/tests/data\",\n    \"licence\": \"Attribution 4.0 International (CC BY 4.0)\",\n}\ndatasource.set_citation(citation=citation)\n</code></pre> <p>You can then get your citation report:</p> <pre><code>datasource.get_citation()\n\n{'author': 'Gavin Chait',\n 'title': 'Portsmouth City Council normalised database of commercial ratepayers',\n 'url': AnyUrl('https://github.com/whythawk/whyqd/tree/master/tests/data', scheme='https', host='github.com', tld='com', host_type='domain', path='/whythawk/whyqd/tree/master/tests/data'),\n 'month': 'feb',\n 'year': 2020,\n 'licence': 'Attribution 4.0 International (CC BY 4.0)'}\n</code></pre> <p>This <code>citation</code> support is available in all <code>Definitions</code>, including SchemaDefinition,  CrosswalkDefinition, TransformDefinition, and DataSourceDefinition.</p>"},{"location":"strategies/schema/","title":"Create and manage a Schema definition","text":"<p>A whyqd (/w\u026ak\u026ad/) <code>schema</code> definition describes the structural organisation of tabular data. Each column is  identified by a field name and defined by conformance to technical specifications. These, along with field constraints  and sensible defaults, ensure interoperability.</p> <p>In simple terms, the columns in source input data must correspond to the fields defined in your schema. These fields  may also be reflected as columns in your database, or analytical software. </p> <p>If your source data conform to your schema definition, you can develop your software and analysis without any data  being present. Data validation with your schema are a separate process and permit you to encapsulate each component of  your project and data management.</p> <p>API</p> <p>Review the <code>class</code> API definitions: SchemaDefinition and CRUDFields.</p>"},{"location":"strategies/schema/#minimum-valid-requirements","title":"Minimum valid requirements","text":"<p>A minimum valid schema requires a <code>name</code> to identify the schema, and a single, minimally-valid <code>field</code> containing a  <code>name</code> and <code>type</code>:</p> <pre><code>{\n\"name\": \"A simple name\",\n\"fields\": [\n{\n\"name\": \"Field name, e.g. 'column_name'\",\n\"type\": \"Valid data type, e.g. 'string', 'number'\"\n}\n]\n}\n</code></pre> <p>Everything else is optional, unless specifically required by that field-type.</p>"},{"location":"strategies/schema/#schema-descriptors","title":"Schema descriptors","text":"<p>Schema terms include:</p>"},{"location":"strategies/schema/#name","title":"<code>name</code>","text":"<p>This is a required term. Spaces will be replaced with <code>_</code> and the string will be lowercased.</p>"},{"location":"strategies/schema/#title","title":"<code>title</code>","text":"<p>A human-readable version of the schema name.</p>"},{"location":"strategies/schema/#description","title":"<code>description</code>","text":"<p>A complete description of the schema. Depending on how complex your work becomes, try and be as helpful as possible to  'future-you'. You'll thank yourself later.</p>"},{"location":"strategies/schema/#missingvalues","title":"<code>missingValues</code>","text":"<p><code>missingValues</code> indicates which string values should be treated as null values. There could be a variety of these, such as '..', or '-'.</p>"},{"location":"strategies/schema/#primarykey","title":"<code>primaryKey</code>","text":"<p>A field or set of fields which uniquely identifies each row in the table. Specify using the <code>name</code> of relevant fields.</p> <p>Data in this field will not be tested for uniqueness. Instead, these data will remain immutable, not being 'forced' into  a date or number type to preserve whatever fruity formatting are described in your input data.</p>"},{"location":"strategies/schema/#index","title":"<code>index</code>","text":"<p>Maximum value of a zero-base index for tabular data defined by this schema. Necessary where <code>actions</code> apply row-level transforms.</p>"},{"location":"strategies/schema/#citation","title":"<code>citation</code>","text":"<p>Full citation for definition. More information can be found in the data source section.</p>"},{"location":"strategies/schema/#version","title":"<code>version</code>","text":"<p>Version and update history for the schema. This is automatically generated when you save the definition. It includes a minimum of <code>updated</code> with the date. Can also include a <code>name</code> for the person producing the version, and a <code>description</code> of the changes or updates made.</p>"},{"location":"strategies/schema/#field-descriptors","title":"Field descriptors","text":"<p>Fields, similarly, contain <code>name</code>, <code>title</code> and <code>description</code>, as well as <code>type</code> as compulsory. The available types are:</p> <ul> <li><code>string</code>: Any text-based string (this is the default),</li> <li><code>number</code>: Any number-based value, including integers and floats,</li> <li><code>integer</code>: Any integer-based value,</li> <li><code>boolean</code>: A boolean [true, false] value. Can set category constraints to fix term used,</li> <li><code>object</code>: Any valid JSON data,</li> <li><code>array</code>: Any valid array-based data,</li> <li><code>date</code>: Any date without a time. Must be in ISO8601 format, <code>YYYY-MM-DD</code>,</li> <li><code>datetime</code>: Any date with a time. Must be in ISO8601 format, with UTC time specified (optionally) as    <code>YYYY-MM-DD hh:mm:ss Zz</code>,</li> <li><code>year</code>: Any year, formatted as <code>YYYY</code>.</li> </ul> <p>To see all the parameter options for the <code>SchemaModel</code>:</p> <pre><code>import whyqd as qd\n\nqd.models.SchemaModel.schema()\n\n  {'title': 'SchemaModel',\n  'type': 'object',\n  'properties': {'uuid': {'title': 'Uuid',\n  'description': 'Automatically generated unique identity for the schema.',\n  'type': 'string',\n  'format': 'uuid'},\n  'name': {'title': 'Name',\n  'description': 'Machine-readable term to uniquely address this schema. Cannot have spaces. CamelCase or snake_case.',\n  'type': 'string'},\n  'title': {'title': 'Title',\n  'description': 'A human-readable version of the schema name.',\n  'type': 'string'},\n  'description': {'title': 'Description',\n  'description': \"A complete description of the schema. Depending on how complex your work becomes, try and be as helpful as possible to 'future-you'. You'll thank yourself later.\",\n  'type': 'string'},\n  'fields': {'title': 'Fields',\n  'description': 'A list of fields which define the schema. Fields, similarly, contain `name`, `title` and `description`, as well as `type` as compulsory.',\n  'default': [],\n  'type': 'array',\n  'items': {'$ref': '#/definitions/FieldModel'}},\n  'version': {'title': 'Version',\n  'description': 'Version and update history for the schema.',\n  'default': [],\n  'type': 'array',\n  'items': {'$ref': '#/definitions/VersionModel'}}},\n  'required': ['name'], ...\n</code></pre> <p>Allowing you to define your initial <code>schema</code>:</p> <pre><code>{\n  \"name\": \"urban_population\",\n  \"title\": \"Urban population\",\n  \"description\": \"Urban population refers to people living in urban areas as defined by national statistical offices. It is calculated using World Bank population estimates and urban ratios from the United Nations World Urbanization Prospects. Aggregation of urban and rural population may not add up to total population because of different country coverages.\",\n}\n</code></pre>"},{"location":"strategies/schema/#name_1","title":"<code>name</code>","text":"<p>This is a required term and is equivalent to a column header. It must be defined exactly as it appears in the tabular  source data.</p> <p>By convention, this should be snake_case with spaces replaced with underscore (e.g. <code>field_1</code>), or as camelCase with  linked words capitalised (e.g. <code>fieldOne</code>). However, given the range of naming conventions, this can only be a  recommendation.</p>"},{"location":"strategies/schema/#title_1","title":"<code>title</code>","text":"<p>A human-readable version of the field name.</p>"},{"location":"strategies/schema/#description_1","title":"<code>description</code>","text":"<p>A complete description of the field. As for the schema, try and be as helpful as possible to future-you.</p>"},{"location":"strategies/schema/#dtype-or-type","title":"<code>dtype</code> or <code>type</code>","text":"<p><code>dtype</code> or <code>type</code> defines the data-type of the field. The core supported types:</p> <ul> <li><code>string</code>: any text-based string.</li> <li><code>number</code>: any number-based value, including integers and floats.</li> <li><code>integer</code>: any integer-based value.</li> <li><code>boolean</code>: a boolean [<code>true</code>, <code>false</code>] value. Can set category constraints to fix term used.</li> <li><code>object</code>: any valid JSON data.</li> <li><code>array</code>: any valid array-based data.</li> <li><code>date</code>: any date without a time. Must be in ISO8601 format, <code>YYYY-MM-DD</code>.</li> <li><code>datetime</code>: any date with a time. Must be in ISO8601 format, with UTC time specified (optionally) as <code>YYYY-MM-DD hh:mm:ss Zz</code>.</li> <li><code>year</code>: any year, formatted as <code>YYYY</code>.</li> </ul> <p>Since the <code>type</code> variable is protected in Python, you'll see it used interchangeably as <code>type</code> or <code>dtype</code> depending on the context. To comply with the JSON Schema definitions, JSON outputs will convert the field name to <code>type</code>.</p>"},{"location":"strategies/schema/#example","title":"<code>example</code>","text":"<p>An example value, as a string, for the field.</p>"},{"location":"strategies/schema/#field-constraints","title":"Field constraints","text":"<p><code>Constraints</code> are optional parameters that act as a primary form of validation. Not all of these are available to every  <code>type</code>, and <code>default_field_settings(type)</code> will list constraints available to a specific field type.</p> <p>Define these as part of your schema definition for a specific field:</p> <pre><code>{\n  \"name\": \"indicator_code\",\n  \"title\": \"Indicator Code\",\n  \"type\": \"string\",\n  \"description\": \"World Bank code reference for Indicator Name.\",\n  \"constraints\": {\"required\": True, \"unique\": True},\n}\n</code></pre> <p>All available constraints:</p> <ul> <li><code>required</code>: <code>boolean</code>, indicates whether this field is compulsory (but blank values in the input column are permitted    and will be set to the missing default)</li> <li><code>unique</code>: <code>boolean</code>, if <code>True</code> then all values for that input column must be unique</li> <li><code>default</code>: Default category (or string) term used when source values are ambiguous, or unstated.</li> <li><code>category</code>: The set of unique category terms permitted in this field,  with <code>name</code> &amp; (optional) <code>description</code>.</li> <li><code>minimum</code>: <code>integer</code> / <code>number</code>, as appropriate defining min number of characters in a string, or the min values of    numbers or integers</li> <li><code>maximum</code>: <code>integer</code> / <code>number</code>, as appropriate defining max number of characters in a string, or the max values of    numbers or integers</li> </ul>"},{"location":"strategies/schema/#category","title":"<code>category</code>","text":"<p><code>Category</code> data are the set of unique category terms permitted in this field. When you define your crosswalk you can define values which should be assigned to each of these categories.</p> <p>In JSON Schema, this is called <code>enum</code>. In the <code>whyqd</code> API, you will refer to <code>.category</code> to  reference the list of categories. However, in the json output files, these will be referenced as <code>enum</code> for compliance with the standard.</p> <p>Define these as part of your schema definition for a specific field:</p> <pre><code>{\n  \"name\": \"test_field\",\n  \"type\": \"string\",\n  \"constraints\": {\n    \"required\": True,\n    \"category\": [\n      {\"name\": \"dog\", \"description\": \"A type of mammal\"},\n      {\"name\": \"cat\", \"description\": \"A different type of mammal\"},\n      {\"name\": \"mouse\", \"description\": \"A small type of mammal\"},\n    ],\n    \"default\": {\"name\": \"dog\", \"description\": \"A type of mammal\"},\n  },\n}\n</code></pre> <p>Each <code>category</code> can have a <code>name</code>, and a <code>description</code>, but the minimum is a <code>name</code>.</p> <p>Each field <code>type</code> will have its own category constraints. For example, boolean categories can use a different term than  True / False defined by the category, but only permits two terms. Others have a minimum of one term in a category, but  require the list member type to be <code>string</code>, <code>number</code>, etc. Ordinarily, <code>category</code> terms must be unique.</p>"},{"location":"strategies/schema/#creating-a-schema","title":"Creating a Schema","text":"<p>The objective of your schema is not only to define a structure for your data, but also provide reference and contextual  information for anyone using it. In a research context, definitions are critical to avoid ambiguity, ensure replication,  and build trust.</p> <p>You can import a schema definition from a file, or you can build it interactively, as shown here. We'll start by  importing whyqd and defining a new schema.</p> <p>The minimum requirement for a schema is that it have a <code>name</code>, but we're going to give it a <code>title</code> and <code>description</code>  as well, because more information is better. We're not barbarians:</p> <p><pre><code>import whyqd as qd\n\nschema: qd.models.SchemaModel = {\n    \"name\": \"urban_population\",\n    \"title\": \"Urban population\",\n    \"description\": \"Urban population refers to people living in urban areas as defined by national statistical offices.\",\n}\nschema_destination = qd.SchemaDefinition()\nschema_destination.set(schema=schema)\n</code></pre> We can also save our schema to a specified <code>directory</code>:</p> <pre><code>directory = \"/path/to/directory\"\nfilename = \"urban_population_2020\"\nschema.save(directory=directory, filename=filename, created_by=\"Gavin Chait\")\n</code></pre> <p>If the optional <code>filename</code> is not provided, the name you specified in the <code>schema</code> dictionary will be used. The file is  a JSON format text file, but will have the extension <code>.schema</code>. A version history will be automatically created, and you  can add your name as <code>created_by</code>.</p> <p>We'll now start to create each of our schema <code>fields</code>.</p> <p>Info</p> <p>You can think of a schema <code>field</code> as a <code>column</code> in a table, or a <code>field</code> in a database. Each field, unsurprisingly,  has a <code>name</code>, <code>title</code> and <code>description</code>, of which only the <code>name</code> is required.</p> <p>Fields have a <code>type</code>, such as <code>number</code> or <code>string</code>. This describes the data expected and limits the actions which  can be performed during the wrangling process</p> <p>We want our destination data to conform to the following structure:</p> la_code ba_ref occupant_name postcode occupation_state occupation_state_date prop_ba_rates occupation_state_reliefs E06000044 177500080710 A company PO5 2SE True 2019-04-01 98530 [small_business, retail] <p>We'll build a single dictionary and then iterate over the list to add each field:</p> <pre><code>import whyqd as qd\n\nschema: qd.models.SchemaModel = {\n    \"name\": \"rates_data\",\n    \"title\": \"Commercial rates data\",\n    \"description\": \"Standardised schema for archival and analysis of commercial / non-domestic rates data.\",\n}\nfields: list[qd.models.FieldModel] = [\n  {\n    \"name\": \"la_code\",\n    \"title\": \"Local authority code\",\n    \"type\": \"string\",\n    \"description\": \"Standard code for local authority.\"\n  },\n  {\n    \"name\": \"ba_ref\",\n    \"title\": \"Billing reference\",\n    \"type\": \"string\",\n    \"description\": \"Unique code for a specific hereditament. May be multiple rows for history.\"\n  },\n  {\n    \"name\": \"prop_ba_rates\",\n    \"title\": \"Property billing rates\",\n    \"type\": \"number\",\n    \"description\": \"Actual rates paid by a specific ratepayer.\"\n  },\n  {\n    \"name\": \"occupant_name\",\n    \"title\": \"Occupier name\",\n    \"type\": \"string\",\n    \"description\": \"Name of the ratepayer.\"\n  },\n  {\n    \"name\": \"postcode\",\n    \"title\": \"Postcode\",\n    \"type\": \"string\",\n    \"description\": \"Full address or postcode of ratepayer.\"\n  },\n  {\n    \"name\": \"occupation_state\",\n    \"title\": \"Occupation state\",\n    \"type\": \"boolean\",\n    \"description\": \"Occupation status, void or occupied.\"\n  },\n  {\n    \"name\": \"occupation_state_date\",\n    \"title\": \"Date of occupation state\",\n    \"type\": \"date\",\n    \"description\": \"Date of the start of status in occupation_state.\"\n  },\n  {\n    \"name\": \"occupation_state_reliefs\",\n    \"title\": \"Occupation state reliefs\",\n    \"type\": \"array\",\n    \"description\": \"Array of the categories of reliefs / exemptions applied.\"\n  }\n]\nschema_destination = qd.SchemaDefinition()\nschema_destination.set(schema=schema)\nschema_destination.fields.add_multi(terms=fields)\n</code></pre> <p>You could also <code>add</code>:</p> <pre><code>for field in fields:\n  schema.fields.add(terms=field)\n</code></pre> <p>From here on we can access any <code>field</code> by calling it by <code>name</code> and then updating it as required:</p> <pre><code>schema.fields.get(name=\"occupation_state_reliefs\")\n\n    {'name': 'occupation_state_reliefs',\n     'type': 'array',\n     'title': 'Occupation state reliefs',\n     'description': 'Array of the categories of reliefs / exemptions applied.'}\n</code></pre> <p>Let's add a list of <code>category</code> terms as a constraint for <code>occupation_state_reliefs</code>:</p> <pre><code>categories = [\"small_business\", \"rural\", \"charity\", \"enterprise_zone\", \"vacancy\", \"hardship\", \"retail\", \"discretionary\", \"exempt\", \"transitional\", \"other\"]\nconstraints = {\n    \"categories\": [{\n      \"name\": category for category in categories\n    }]\n  }\nschema.fields.set_constraints(name=\"occupation_state_reliefs\", constraints=constraints)\nschema.fields.get(name=\"occupation_state_reliefs\").dict(by_alias=True, exclude_defaults=True, exclude_none=True)\n\n  {'uuid': UUID('cf4d066e-22a8-4b76-8956-f6120eec4c52'),\n  'name': 'occupation_state_reliefs',\n  'title': 'Occupation state reliefs',\n  'description': 'Array of the categories of reliefs / exemptions applied.',\n  'type': 'array',\n  'constraints': {'enum': [{'uuid': UUID('daa206a9-ac8c-41a9-a504-06410780ee50'),\n    'name': 'small_business'},\n  {'uuid': UUID('5964e9fc-dd50-4856-acdc-2326ea48ef1d'), 'name': 'rural'},\n  {'uuid': UUID('498654f9-8825-4f3d-a573-0c110726fba4'), 'name': 'charity'},\n  {'uuid': UUID('f94353ce-a489-4fb1-ad78-5435b3dd54a4'),\n    'name': 'enterprise_zone'},\n  {'uuid': UUID('41285fc0-2321-4542-b7f1-e8e535588559'), 'name': 'vacancy'},\n  {'uuid': UUID('28068ff2-15ff-409a-9a8f-f97c39407812'), 'name': 'hardship'},\n  {'uuid': UUID('b8041d21-f8ca-47b9-b3fe-7b9077388459'), 'name': 'retail'},\n  {'uuid': UUID('83bda0d4-3d94-4738-a580-cfe0881c8e4d'),\n    'name': 'discretionary'},\n  {'uuid': UUID('ff2cbc0c-839b-430c-bdca-ac4238634f05'), 'name': 'exempt'},\n  {'uuid': UUID('f4300571-c04b-4cbf-b835-16c5ae3343b0'),\n    'name': 'transitional'},\n  {'uuid': UUID('8a3af6f4-f48c-4614-83f2-ba472b2129e9'), 'name': 'other'}]}}\n</code></pre> <p>The term <code>.dict(by_alias=True, exclude_defaults=True, exclude_none=True)</code> is used to extract a dictionary format from  the underlying Pydantic model used by <code>whyqd</code>.</p> <p>Info</p> <p>These are the official business rates reliefs permitted by the  UK government. Unsurprisingly, only by accident do any local authorities actually use these terms when awarding a  relief.</p> <p>Review your schema, then <code>save</code> and we're ready to begin schema-to-schema conversion:</p> <pre><code>schema.get.dict(by_alias=True, exclude_defaults=True, exclude_none=True)\n\n    {'uuid': UUID('19692345-2caf-46b1-9a8f-276491520c6b'),\n    'name': 'test_schema',\n    'title': 'Test Schema',\n    'description': 'A test Schema',\n    'fields': [{'uuid': UUID('615d2cd0-f8b6-4449-b3d2-642fa4836888'),\n    'name': 'la_code',\n    'title': 'Local authority code',\n    'description': 'Standard code for local authority.',\n    'type': 'string',\n    'constraints': {'default': {'uuid': UUID('579342cd-bba8-41cd-bf45-3c517b8cd75e'),\n        'name': 'E06000044'}}},\n    {'uuid': UUID('95f5c53c-59e1-4bb7-917d-7177b01d2d3c'),\n    'name': 'ba_ref',\n    'title': 'Billing reference',\n    'description': 'Unique code for a specific hereditament. May be multiple rows for history.',\n    'type': 'string'},\n    {'uuid': UUID('7572ae3e-d725-4897-84fb-5c5b45bd4edb'),\n    'name': 'prop_ba_rates',\n    'title': 'Property billing rates',\n    'description': 'Actual rates paid by a specific ratepayer.',\n    'type': 'number'},\n    {'uuid': UUID('ac76c3ab-5ef8-4641-99ec-aab2c5b7414c'),\n    'name': 'occupant_name',\n    'title': 'Occupier name',\n    'description': 'Name of the ratepayer.',\n    'type': 'string'},\n    {'uuid': UUID('26440eba-fd1d-40af-a52c-a9351fad2fd9'),\n    'name': 'postcode',\n    'title': 'Postcode',\n    'description': 'Full address or postcode of ratepayer.',\n    'type': 'string'},\n    {'uuid': UUID('28d7863b-22fa-4bd5-a221-0607643f0111'),\n    'name': 'occupation_state',\n    'title': 'Occupation state',\n    'description': 'Occupation status, void or occupied.',\n    'type': 'boolean',\n    'constraints': {'enum': [{'uuid': UUID('353bd4ac-d677-47c4-af40-6f651af2cc5e'),\n        'name': True},\n        {'uuid': UUID('33f8b2f8-9ac5-412a-9507-879bb7f845ce'), 'name': False}],\n        'default': {'uuid': UUID('353bd4ac-d677-47c4-af40-6f651af2cc5e'),\n        'name': True}}},\n    {'uuid': UUID('79a70822-4e24-4a68-9036-992def200cd6'),\n    'name': 'occupation_state_date',\n    'title': 'Date of occupation state',\n    'description': 'Date of the start of status in occupation_state.',\n    'type': 'date'},\n    {'uuid': UUID('cf4d066e-22a8-4b76-8956-f6120eec4c52'),\n    'name': 'occupation_state_reliefs',\n    'title': 'Occupation state reliefs',\n    'description': 'Array of the categories of reliefs / exemptions applied.',\n    'type': 'array',\n    'constraints': {'enum': [{'uuid': UUID('daa206a9-ac8c-41a9-a504-06410780ee50'),\n        'name': 'small_business'},\n        {'uuid': UUID('5964e9fc-dd50-4856-acdc-2326ea48ef1d'), 'name': 'rural'},\n        {'uuid': UUID('498654f9-8825-4f3d-a573-0c110726fba4'), 'name': 'charity'},\n        {'uuid': UUID('f94353ce-a489-4fb1-ad78-5435b3dd54a4'),\n        'name': 'enterprise_zone'},\n        {'uuid': UUID('41285fc0-2321-4542-b7f1-e8e535588559'), 'name': 'vacancy'},\n        {'uuid': UUID('28068ff2-15ff-409a-9a8f-f97c39407812'),\n        'name': 'hardship'},\n        {'uuid': UUID('b8041d21-f8ca-47b9-b3fe-7b9077388459'), 'name': 'retail'},\n        {'uuid': UUID('83bda0d4-3d94-4738-a580-cfe0881c8e4d'),\n        'name': 'discretionary'},\n        {'uuid': UUID('ff2cbc0c-839b-430c-bdca-ac4238634f05'), 'name': 'exempt'},\n        {'uuid': UUID('f4300571-c04b-4cbf-b835-16c5ae3343b0'),\n        'name': 'transitional'},\n        {'uuid': UUID('8a3af6f4-f48c-4614-83f2-ba472b2129e9'),\n        'name': 'other'}]}}]}\n\nschema.save(directory=directory, filename=filename, created_by=\"Gavin Chait\")\n</code></pre> <p>Whyqd's data source strategies show you how to derive a schema to reflect source data.</p>"},{"location":"tutorials/tutorial1/","title":"Tutorial 1: Aligning multiple sources of local government data to a single schema","text":"<p>whyqd (/w\u026ak\u026ad/) was developed to solve a daily, grinding need. This tutorial is based on a real-world problem.</p> <p>Learning outcomes</p> <ul> <li>Develop and apply a transformation strategy</li> <li>Design and create a standardised destination schema</li> <li>Extract source data and derived individual source schemas</li> <li>Perform crosswalks and validations</li> </ul> <p>Source data are from Portsmouth City Council and it is assumed you have familiarity with Python and Pydantic.</p>"},{"location":"tutorials/tutorial1/#background","title":"Background","text":"<p>Our openLocal.uk project is a quarterly-updated commercial location database, aggregating open  data on vacancies, rental valuations, rates and ratepayers, into an integrated time-series database of individual  retail, industrial, office and leisure business units. </p> <p>Every three months, we import about 300 very messy spreadsheets from local authorities across the UK. These need to be  restructured to conform to a single schema, including redefining whatever weird terms they use to describe categorical  data, and only then can we begin the automated process of cleaning and validation.</p> <p>Our work is mainly used by researchers and the UK government but, during COVID in 2020-21, these data took on political and economic heft when they were part of the research base informing impact assessment for the various lockdowns and  the economic recovery afterwards.</p> <p>Levelling Up awarded almost \u00a34 billion to projects across the UK. But for some to get, others had their requests refused.</p> <p>The rejected began to aggressively question the entire evidence supporting the allocation, and openLocal was targeted.  The strategy and approach presented here made it straightforward for us to demonstrate how we ensure data probity. We  wrote up our methods.</p> <p>Curation makes your workflow more efficient, but it can also keep you out of legal peril.</p>"},{"location":"tutorials/tutorial1/#strategy","title":"Strategy","text":"<p>Strategy</p> <p>Our objective is a sequential event-based data series, capturing everything that happened for every commercial  address in a particular source authority. Since multiple things can happen on a single date which may be presented in an ambiguous way from the source, we may end up \"losing\" some data but as long as we can recover information  from the sequence, we're good.</p> <p>Our source data can be in any tabular format (XLS, XLSX and CSV). There are no common rates data management systems, no  agreed schema or terminology, and high staff turnover means change in data structure from the same sources between  releases.</p> <p>Given this diversity, we have a single acceptance criterion to include a source in an update process: it must include a field for the data we use as a foreign key for cross-data merging. It doesn't matter if it's in poor quality, but it must be there.</p> <p>We also need a single destination schema which is common across the project.</p> <p>If we're really lucky, our data source don't change their definitions or structure from release-to-release and we can  reuse our crosswalks. In our experience that is like searching for unicorns.</p>"},{"location":"tutorials/tutorial1/#define-a-destination-schema","title":"Define a destination schema","text":"<p>We want our destination data to conform to the following structure:</p> la_code ba_ref occupant_name postcode occupation_state occupation_state_date prop_ba_rates occupation_state_reliefs E06000044 177500080710 A company PO5 2SE True 2019-04-01 98530 [small_business, retail] <p>Review the schema documentation for more details, but these are the <code>type</code> of data we need here:</p> <ul> <li><code>string</code>: any text-based string.</li> <li><code>number</code>: any number-based value, including integers and floats.</li> <li><code>boolean</code>: a boolean [<code>True</code>, <code>False</code>] value. Can set category constraints to fix term used.</li> <li><code>array</code>: any valid array-based data (used for a list of categorical terms).</li> <li><code>date</code>: any date without a time. Must be in ISO8601 format, <code>YYYY-MM-DD</code>.</li> </ul> <p>In addition, these data can be <code>constrained</code>:</p> <ul> <li><code>required</code>: boolean, indicates whether this field is compulsory (but blank values in the input column are permitted    and will be set to the missing default),</li> <li><code>category</code>: the set of unique category terms permitted in this field.</li> </ul> <p>Let's start:</p> <pre><code>import whyqd as qd\n\nschema_destination = qd.SchemaDefinition()\n# Using Pydantic model validation\nschema: qd.models.SchemaModel = {\n  \"name\": \"rates_data_schema\",\n  \"title\": \"UK Ratepayer data schema\",\n  \"description\": \"Structural metadata target for imported messy data from the 348 local authorities in England &amp; Wales.\"\n}\nschema_destination.set(schema=schema)\n</code></pre> <p>We'll build a single fields dictionary and then iterate over the list to add each field:</p> <pre><code>fields = [\n  {\n    \"name\": \"la_code\",\n    \"title\": \"Local authority code\",\n    \"type\": \"string\",\n    \"description\": \"Standard code for local authority.\"\n  },\n  {\n    \"name\": \"ba_ref\",\n    \"title\": \"Billing reference\",\n    \"type\": \"string\",\n    \"description\": \"Unique code for a specific hereditament. May be multiple rows for history.\"\n  },\n  {\n    \"name\": \"prop_ba_rates\",\n    \"title\": \"Property billing rates\",\n    \"type\": \"number\",\n    \"description\": \"Actual rates paid by a specific ratepayer.\"\n  },\n  {\n    \"name\": \"occupant_name\",\n    \"title\": \"Occupier name\",\n    \"type\": \"string\",\n    \"description\": \"Name of the ratepayer.\"\n  },\n  {\n    \"name\": \"postcode\",\n    \"title\": \"Postcode\",\n    \"type\": \"string\",\n    \"description\": \"Full address or postcode of ratepayer.\"\n  },\n  {\n    \"name\": \"occupation_state\",\n    \"title\": \"Occupation state\",\n    \"type\": \"boolean\",\n    \"description\": \"Occupation status, void or occupied.\"\n  },\n  {\n    \"name\": \"occupation_state_date\",\n    \"title\": \"Date of occupation state\",\n    \"type\": \"date\",\n    \"description\": \"Date of the start of status in occupation_state.\"\n  },\n  {\n    \"name\": \"occupation_state_reliefs\",\n    \"title\": \"Occupation state reliefs\",\n    \"type\": \"array\",\n    \"description\": \"Array of the categories of reliefs / exemptions applied.\"\n  }\n]\nschema_destination.fields.add_multi(terms=fields)\n</code></pre> <p>From here on we can access any <code>field</code> by calling it by <code>name</code> and then updating it as required:</p> <pre><code>schema_destination.fields.get(name=\"occupation_state_reliefs\")\n</code></pre> <p>Let's add a list of <code>category</code> terms as a constraint for <code>occupation_state_reliefs</code>. These are derived from the official  business rates reliefs:</p> <pre><code>categories = [\n  \"small_business\", \"rural\", \"charity\", \"enterprise_zone\", \"vacancy\", \n  \"hardship\", \"retail\", \"discretionary\", \"exempt\", \"transitional\", \"other\"\n]\nconstraints = {\n    \"categories\": [{\n      \"name\": category for category in categories\n    }]\n  }\nschema_destination.fields.set_constraints(name=\"occupation_state_reliefs\", constraints=constraints)\n</code></pre> <p>Use <code>.dict(by_alias=True, exclude_defaults=True, exclude_none=True)</code> to extract a dictionary format from the underlying Pydantic model.</p> <pre><code>schema_destination.fields.get(name=\"occupation_state_reliefs\"\n                              ).dict(\n                                by_alias=True, \n                                exclude_defaults=True, \n                                exclude_none=True)\n\n{\n  'uuid': UUID('cf4d066e-22a8-4b76-8956-f6120eec4c52'),\n  'name': 'occupation_state_reliefs',\n  'title': 'Occupation state reliefs',\n  'description': 'Array of the categories of reliefs / exemptions applied.',\n  'type': 'array',\n  'constraints': {\n    'enum': [\n      {\n        'uuid': UUID('daa206a9-ac8c-41a9-a504-06410780ee50'),\n        'name': 'small_business'\n      },\n      {\n        'uuid': UUID('5964e9fc-dd50-4856-acdc-2326ea48ef1d'), \n        'name': 'rural'\n      },\n      {\n        'uuid': UUID('498654f9-8825-4f3d-a573-0c110726fba4'), \n        'name': 'charity'\n      },\n      {\n        'uuid': UUID('f94353ce-a489-4fb1-ad78-5435b3dd54a4'),\n        'name': 'enterprise_zone'\n      },\n      {\n        'uuid': UUID('41285fc0-2321-4542-b7f1-e8e535588559'), \n        'name': 'vacancy'\n      },\n      {\n        'uuid': UUID('28068ff2-15ff-409a-9a8f-f97c39407812'), \n        'name': 'hardship'\n      },\n      {\n        'uuid': UUID('b8041d21-f8ca-47b9-b3fe-7b9077388459'), \n        'name': 'retail'\n      },\n      {\n        'uuid': UUID('83bda0d4-3d94-4738-a580-cfe0881c8e4d'),\n        'name': 'discretionary'\n      },\n      {\n        'uuid': UUID('ff2cbc0c-839b-430c-bdca-ac4238634f05'), \n        'name': 'exempt'\n      },\n      {\n        'uuid': UUID('f4300571-c04b-4cbf-b835-16c5ae3343b0'),\n        'name': 'transitional'\n      },\n      {\n        'uuid': UUID('8a3af6f4-f48c-4614-83f2-ba472b2129e9'), \n        'name': 'other'\n      }\n    ]\n  }\n}\n</code></pre> <p>This is our curation foundation and we can save it, ensuring citation and version control.</p> <pre><code>schema_destination.save(directory=DIRECTORY, filename=FILENAME, created_by=CREATOR)\n</code></pre> <p>We'll reference this definition saved source path as <code>SCHEMA_DESTINATION</code> in the rest of the tutorial.</p>"},{"location":"tutorials/tutorial1/#source-data-and-source-schema-definitions","title":"Source data and source schema definitions","text":"<p>Portsmouth data are reasonably well-structured:</p> <p></p> <p>However, once you import and derive its data model you'll discover a slight hiccough:</p> <pre><code>datasource = qd.DataSourceDefinition()\ndatasource.derive_model(source=SOURCE_DATA, mimetype=MIMETYPE)\n\nif isinstance(datasource.get, list):\n  print(len(datasource.get))\n\n3\n</code></pre> <p>If you look at the spreadsheet at the same time, you'll see it's an Excel file with three tabs. whyqd has automatically imported all three, and derived the data model for each. Let's review:</p> <ul> <li>Each data model has a different set of columns, meaning there is no common source schema or crosswalk,</li> <li>There is a foreign key for each, so they meet our minimum acceptance criteria.</li> </ul> <p>Turns out you don't have one transformation, you need to do three.</p> <p>Now technically, because there are so many common fields, we could create a single source schema and use it for all three source tabs, but let's use a verbose approach for this tutorial and create three separate source schemas.</p> <p>We also recognise that two columns are used to define our categorical data. We need to extract the terms used in these columns so we can assign them appropriately later:</p> <pre><code>CATEGORY_FIELDS = [\"Current Relief Type\", \"Current Property Exemption Code\"]\nSCHEMA_SOURCE = {}\nDATA_SOURCE = {}\n\nfor ds in datasource.get:\n    DATA_SOURCE[ds.sheet_name] = ds\n    schema_source = qd.SchemaDefinition(source={\n          \"name\": f\"portsmouth-{ds.sheet_name.lower()}\"\n    })\n    schema_source.derive_model(data=ds)\n    field_names = [c.name for c in ds.columns]\n    if not set(CATEGORY_FIELDS).isdisjoint(field_names):\n        # we don't want to load the data if don't need\n        # whyqd will only load the needed sheet as defined in the data model\n        df = datasource.reader.get(source=ds)\n        for category_field in CATEGORY_FIELDS:\n            if category_field in df.columns:\n                schema_source.fields.set_categories(name=category_field, terms=df)\n    SCHEMA_SOURCE[ds.sheet_name] = schema_source\n</code></pre> <p>Leaving us with a list of source schema, along with their appropriate categories:</p> <pre><code>SCHEMA_SOURCE[\"Report1\"].get.dict(by_alias=True, exclude_defaults=True, exclude_none=True)\n\n{'uuid': UUID('a25091ef-2bad-4207-8e04-dc50760de5f9'),\n 'name': 'portsmouth-report1',\n 'fields': [{'uuid': UUID('b5a4592c-46a3-41fb-aa22-9f48400e09ae'),\n   'name': 'Property Reference Number',\n   'title': 'Property Reference Number',\n   'type': 'string'},\n  {'uuid': UUID('3c31d709-4c37-4830-9781-391bf3a990cb'),\n   'name': 'Primary Liable party name',\n   'title': 'Primary Liable party name',\n   'type': 'string'},\n  {'uuid': UUID('86145ff6-973f-4f6e-85ce-f5c73d3da9ca'),\n   'name': 'Full Property Address',\n   'title': 'Full Property Address',\n   'type': 'string'},\n  {'uuid': UUID('f6690650-908b-4b1c-90e1-1f81470bb7e4'),\n   'name': 'Current Relief Type',\n   'title': 'Current Relief Type',\n   'type': 'string',\n   'constraints': {'enum': [{'uuid': UUID('619941f4-719c-43d9-be70-30343366e51d'),\n      'name': 'Retail Discount'},\n     {'uuid': UUID('dcaccfdc-cdec-4e60-a3ab-bcade51260cf'),\n      'name': 'Small Business Relief England'},\n     {'uuid': UUID('436bc5c4-c627-4b99-b82e-cde5c2f8a329'),\n      'name': 'Mandatory'},\n     {'uuid': UUID('0e228796-815b-404e-99e8-c19572e6a414'),\n      'name': 'Empty Property Rate Non-Industrial'},\n     {'uuid': UUID('76f56e35-9c70-4d5c-85ce-c8fe61479944'),\n      'name': 'Empty Property Rate Industrial'},\n     {'uuid': UUID('3d1b5668-f14e-42a8-80f3-ca60b0cccbc3'),\n      'name': 'Supporting Small Business Relief'},\n     {'uuid': UUID('1deb0ab9-b630-4b61-8fb6-8a78fea8dfac'),\n      'name': 'Sports Club (Registered CASC)'},\n     {'uuid': UUID('a8ad1f3f-fea6-4e2f-b5cf-000c0b53e900'),\n      'name': 'Sbre Extension For 12 Months'},\n     {'uuid': UUID('01c067be-3ce9-4059-a8dd-c0bcfe664fc3'),\n      'name': 'Empty Property Rate Charitable'}]}},\n  {'uuid': UUID('ab9a4381-657c-419d-995f-28d2ec1eda87'),\n   'name': 'Account Start date',\n   'title': 'Account Start date',\n   'type': 'string'},\n  {'uuid': UUID('646743e4-b70b-4f9d-b891-c856b7755296'),\n   'name': 'Current Relief Award Start Date',\n   'title': 'Current Relief Award Start Date',\n   'type': 'string'},\n  {'uuid': UUID('4f814e7a-8c29-4736-a7de-59a54b138236'),\n   'name': 'Current Rateable Value',\n   'title': 'Current Rateable Value',\n   'type': 'string'}],\n 'attributes': {'rowCount': 3421}}\n</code></pre>"},{"location":"tutorials/tutorial1/#defining-crosswalks","title":"Defining crosswalks","text":"<p>Fortunately, this isn't a particularly difficult crosswalk, with only the categorisations requiring any complexity, and we need to create one column for the local authority itself so that we can track each dataset when we do the eventual merge into the database.</p> <p>Portsmouth has the census code <code>E06000044</code> which we use as an additional foreign key. </p> <p>Let's define our three crosswalks and then review:</p> <pre><code>SCHEMA_SCRIPTS = {\n    \"Report1\": [\n        \"NEW &gt; 'la_code' &lt; ['E06000044']\",\n        \"RENAME &gt; 'ba_ref' &lt; ['Property Reference Number']\",\n        \"RENAME &gt; 'prop_ba_rates' &lt; ['Current Rateable Value']\",\n        \"RENAME &gt; 'occupant_name' &lt; ['Primary Liable party name']\",\n        \"RENAME &gt; 'postcode' &lt; ['Full Property Address']\",\n        \"CATEGORISE &gt; 'occupation_state'::False &lt; 'Current Relief Type'::['Empty Property Rate Non-Industrial', 'Empty Property Rate Industrial', 'Empty Property Rate Charitable']\",\n        \"CATEGORISE &gt; 'occupation_state_reliefs'::'small_business' &lt; 'Current Relief Type'::['Small Business Relief England', 'Sbre Extension For 12 Months', 'Supporting Small Business Relief']\",\n        \"CATEGORISE &gt; 'occupation_state_reliefs'::'vacancy' &lt; 'Current Relief Type'::['Empty Property Rate Non-Industrial', 'Empty Property Rate Industrial', 'Empty Property Rate Charitable']\",\n        \"CATEGORISE &gt; 'occupation_state_reliefs'::'retail' &lt; 'Current Relief Type'::['Retail Discount']\",\n        \"CATEGORISE &gt; 'occupation_state_reliefs'::'other' &lt; 'Current Relief Type'::['Sports Club (Registered CASC)', 'Mandatory']\",\n        \"SELECT_NEWEST &gt; 'occupation_state_date' &lt; ['Current Relief Award Start Date' + 'Current Relief Award Start Date', 'Account Start date' + 'Account Start date']\",\n    ],\n    \"Report2\": [\n        \"NEW &gt; 'la_code' &lt; ['E06000044']\",\n        \"RENAME &gt; 'ba_ref' &lt; ['Property Reference Number']\",\n        \"RENAME &gt; 'prop_ba_rates' &lt; ['Current Rateable Value']\",\n        \"RENAME &gt; 'occupant_name' &lt; ['Primary Liable party name']\",\n        \"RENAME &gt; 'postcode' &lt; ['Full Property Address']\",\n        \"CATEGORISE &gt; 'occupation_state'::False &lt; 'Current Property Exemption Code'::['EPRN', 'EPRI', 'VOID', 'EPCH', 'LIQUIDATE', 'DECEASED', 'PROHIBITED', 'BANKRUPT']\",\n        \"CATEGORISE &gt; 'occupation_state_reliefs'::'enterprise_zone' &lt; 'Current Property Exemption Code'::['INDUSTRIAL']\",\n        \"CATEGORISE &gt; 'occupation_state_reliefs'::'vacancy' &lt; 'Current Property Exemption Code'::['EPRN', 'EPRI', 'VOID', 'EPCH', 'LIQUIDATE', 'DECEASED', 'PROHIBITED', 'BANKRUPT']\",\n        \"CATEGORISE &gt; 'occupation_state_reliefs'::'exempt' &lt; 'Current Property Exemption Code'::['C', 'LOW RV', 'LAND']\",\n        \"RENAME &gt; 'occupation_state_date' &lt; ['Current Prop Exemption Start Date']\",\n    ],\n    \"Report3\": [\n        \"NEW &gt; 'la_code' &lt; ['E06000044']\",\n        \"RENAME &gt; 'ba_ref' &lt; ['Property ref no']\",\n        \"RENAME &gt; 'prop_ba_rates' &lt; ['Current Rateable Value']\",\n        \"RENAME &gt; 'occupant_name' &lt; ['Primary Liable party name']\",\n        \"RENAME &gt; 'postcode' &lt; ['Full Property Address']\",\n        \"RENAME &gt; 'occupation_state_date' &lt; ['Account Start date']\",\n    ],\n}\n</code></pre> <p>Remember the basic structure of an <code>script</code>: <code>ACTION &gt; DESTINATION &lt; [SOURCE]</code>.</p> <p>We're using only four types of action. <code>NEW</code> and <code>RENAME</code> should be self-explanatory, but let's look at two the others.</p> <p>The <code>Report1</code> tab has two date fields. One for the date a specific relief was awarded, and the other for when the  ratepayer became responsible for the account. It may happen that a ratepayer starts on the same day they receive a relief. They may also be discrete events, meaning that there'd be an entry when they become the account-holder and  another when they are awarded the relief. We get both dates on each row. How to choose which is relevant?</p> <pre><code>\"\"\"\nSELECT_NEWEST &gt; 'occupation_state_date' \n          &lt; \n          ['Current Relief Award Start Date' + 'Current Relief Award Start Date', \n          'Account Start date' + 'Account Start date']\n\"\"\"\n</code></pre> <p><code>SELECT_NEWEST</code> means take the latest field from the list of fields. We create a list of source choices with a <code>MODIFIER</code>.</p> <p>The format is <code>'target_field' + 'date_field'</code>. <code>SELECT_NEWEST</code> will compare the <code>date_field</code> terms, pick the latest, and  assign it's associated <code>target_field</code> to the <code>destination_field</code>. In our case, both <code>target_field</code> and <code>date_field</code>  are the same field. We want the most recent date from the choice of each column.</p> <p>We have two categorical destination fields. <code>occupation_state</code> takes a <code>boolean</code> category, while <code>occupation_state_reliefs</code> requires a list of terms.</p> <p><code>CATEGORISE</code> has the form: </p> <pre><code>CATEGORISE &gt; 'destination_field'::'destination_category' \n           &lt; 'source_field'::['source_category', source_category']\n</code></pre> <p>The <code>destination_category</code> can be boolean, and we need to be sure we understand whether the source data defines an  address as occupied (<code>True</code>) or vacant / void (<code>False</code>). Getting things the wrong with booleans is a frustratingly  common curation error.</p> <p>After this, it all comes together very quickly.</p>"},{"location":"tutorials/tutorial1/#transformations-with-crosswalks","title":"Transformations with crosswalks","text":"<p>We can now loop through our source schema, assign its corresponding crosswalk and produce transformations:</p> <pre><code>for key, schema_source in SCHEMA_SOURCE.items():\n    # Define a Crosswalk\n    crosswalk = qd.CrosswalkDefinition()\n    crosswalk.set(schema_source=schema_source, schema_destination=SCHEMA_DESTINATION)\n    crosswalk.actions.add_multi(terms=SCHEMA_SCRIPTS[key])\n    # Transform a data source\n    transform = qd.TransformDefinition(crosswalk=crosswalk, data_source=DATA_SOURCE[key])\n    transform.process()\n    transform.save(directory=DIRECTORY)\n    # Validate a data source\n    DESTINATION_DATA = DIRECTORY / transform.model.dataDestination.name\n    TRANSFORM = DIRECTORY / f\"{transform.model.name}.transform\"\n    valiform = qd.TransformDefinition()\n    valiform.validate(\n        transform=TRANSFORM, data_destination=DESTINATION_DATA, mimetype_destination=DESTINATION_MIMETYPE\n    )\n</code></pre> <p>The default transformation destination data saved mimetype is <code>parquet</code>. You can specify something else.</p> <p>The validation step isn't necessary, but is a sanity check. It will compare the result of your transform by rerunning the script and comparing the result to your original save destination file.</p> <p>Despite all this coding and activity, you've actually made no changes to the source data. Everything you've done has  been about documenting a process. This process is the only thing that will eventually execute and produce your output.</p>"},{"location":"tutorials/tutorial1/#preparing-a-citation","title":"Preparing a Citation","text":"<p>Research-based data scientists are not always treated well in the research community. Data are hoarded by researchers,  which also means that the people who produced that data don't get referenced or recognised.</p> <p>whyqd is designed to support a research process and ensure citation of the incredible work done by research-based data scientists.</p> <p>A citation is a special set of fields, with options for:</p> <ul> <li>author: The name(s) of the author(s) (in the case of more than one author, separated by <code>and</code>),</li> <li>title: The title of the work,</li> <li>url: The URL field is used to store the URL of a web page or FTP download. It is a non-standard BibTeX field,</li> <li>publisher: The publisher's name,</li> <li>institution: The institution that was involved in the publishing, but not necessarily the publisher,</li> <li>doi: The doi field is used to store the digital object identifier (DOI) of a journal article, conference paper,   book chapter or book. It is a non-standard BibTeX field. It's recommended to simply use the DOI, and not a DOI link,</li> <li>month: The month of publication (or, if unpublished, the month of creation). Use three-letter abbreviation,</li> <li>year: The year of publication (or, if unpublished, the year of creation),</li> <li>licence: The terms under which the associated resource are licenced for reuse. It is a non-standard BibTeX field,</li> <li>note: Miscellaneous extra information.</li> </ul> <p>Let's set up a citation for this tutorial:</p> <pre><code>citation = {\n            \"author\": \"Gavin Chait\",\n            \"month\": \"feb\",\n            \"year\": 2020,\n            \"title\": \"Portsmouth City Council normalised database of commercial ratepayers\",\n            \"url\": \"https://github.com/whythawk/whyqd/tree/master/tests/data\",\n            \"licence\": \"Attribution 4.0 International (CC BY 4.0)\"\n        }\ntransform.set_citation(citation=citation)\n</code></pre> <p>You can then get your citation report:</p> <pre><code>transform.get_citation()\n\n{'author': 'Gavin Chait',\n'title': 'Portsmouth City Council normalised database of commercial ratepayers',\n'url': AnyUrl('https://github.com/whythawk/whyqd/tree/master/tests/data', scheme='https', host='github.com', tld='com', host_type='domain', path='/whythawk/whyqd/tree/master/tests/data'),\n'month': 'feb',\n'year': 2020,\n'licence': 'Attribution 4.0 International (CC BY 4.0)'}\n</code></pre> <p>Those of you familiar with Dataverse's universal numerical fingerprint may be wondering where it is? whyqd, similarly, produces a unique hash for each datasource. Ours is based on  BLAKE2b and is included in the data source model saved in the  transform.</p> <pre><code>transform.get.dataDestination.checksum\n\n'a8d03afd7d5b93163dac56ba23a7c75dedf42b8999295e560f3d633d54457e9de3ae95dea8181f238e549a7fba10a36723d2f1b1d94ef3f2273129a58bfc0751'\n</code></pre>"},{"location":"tutorials/tutorial1/#extending-the-tutorial","title":"Extending the tutorial","text":"<p>There are three source data schema, but these share a fair number of common fields. They're not ambiguous, either. The spelling and definitions are identical. You could merge these into one and then any imported datasource could use a single source schema. You could even work out a way to add source-specific crosswalk scripts algorithmically (I'm  thinking of the categorical scripts, and the ordering of multi-date fields).</p> <p>This is left to you as an exercise.</p>"},{"location":"tutorials/tutorial2/","title":"Tutorial 2: Pivoting wide-format data into archival long-format","text":"<p>whyqd (/w\u026ak\u026ad/) helps with what is a frequent tension in data communication. The way we archive data is different from the way people need to see or use it.</p> <p>Learning outcomes</p> <ul> <li>Develop a sense of the difference between archival and presentation data</li> <li>Demonstrate how to define a schema</li> <li>Perform pivot-based crosswalks and generate a schema-compliant output</li> </ul> <p>Source data are from World Bank urban population time-series and it is assumed you have familiarity with Python and Pydantic.</p>"},{"location":"tutorials/tutorial2/#strategy","title":"Strategy","text":"<p>Strategy</p> <p>Curation is about recognising contextual requirements. Updating rows in a database is far easier than adding in new columns / fields. Yet people find it easier to \"read\" data like text (so in the direction of travel - right-to-left or left-to-right, depending on language).</p> <p>This often means you need to  pivot data from wide- to long- formats, and vice versa.</p> <p>One of the most obvious uses for wide-format data is for visualisation. Libraries like D3.js expect data-series in lists. The World Bank has gotten into the habit of making their open data for their time-series data wide. Which is fine, but every year it gets wider.</p> <p>For archival, that's a problem. Instead of adding a few new rows, you would need to update your entire database; first add in a new year / value field, and then updating every row with data for the new field. That's frought with risk for destructive change to source data.</p> <p>Instead we will simply clean up and pivot the source data.</p> <p>There is an additional problem. Country names.</p> <p>Strategy</p> <p>There is an almost perverse number of ways of spelling country names. </p> <p>Take C\u00f4te d'Ivoire, which can be spelled as C\u00f4te d'Ivoire, Cote d'Ivoire, Republic of C\u00f4te d'Ivoire, or - if  people are feeling extra churlish - Ivory Coast.</p> <p>And countries can decide to change their names too. Turkey is now known as T\u00fcrkiye.</p> <p>Good practice is to treat country names as categorical data so that you can rematch as required without needing an additional post-transformation step.</p> <p>We won't be doing that here, merely to keep the tutorial shorter (you can review the first tutorial for guidance on categorical data assignment). We have a <code>country_code</code> field which does keep things ordered, so the  <code>country_name</code> field is mostly a human-aid. </p> <p>Keep it in mind, though. Consistent naming is a perpetual challenge.</p>"},{"location":"tutorials/tutorial2/#define-a-destination-schema","title":"Define a destination schema","text":"<p>We want our destination data to conform to the following structure:</p> indicator_code indicator_name country_code country_name year values SP.URB.TOTL Urban population ABW Aruba 1960 27526 SP.URB.TOTL Urban population AFG Afghanistan 1960 755836 SP.URB.TOTL Urban population AGO Angola 1960 569222 SP.URB.TOTL Urban population ALB Albania 1960 493982 <p>Review the schema documentation for more details, but these are the <code>type</code> of data we need:</p> <ul> <li><code>string</code>: any text-based string.</li> <li><code>number</code>: any number-based value, including integers and floats.</li> <li><code>year</code>: Any year, formatted as <code>YYYY</code>.</li> </ul> <p>In addition, these data can be <code>constrained</code>:</p> <ul> <li><code>required</code>: boolean, indicates whether this field is compulsory.</li> </ul> <p>Let's start:</p> <pre><code>import whyqd as qd\n\nschema_destination = qd.SchemaDefinition()\n# Using Pydantic model validation\nschema: qd.models.SchemaModel = {\n    \"name\": \"urban_population\",\n    \"title\": \"Urban population\",\n    \"description\": \"Urban population refers to people living in urban areas as defined by national statistical offices. It is calculated using World Bank population estimates and urban ratios from the United Nations World Urbanization Prospects. Aggregation of urban and rural population may not add up to total population because of different country coverages.\",\n}\nschema_destination.set(schema=schema)\n</code></pre> <p>We'll build a single fields dictionary and then iterate over the list to add each field:</p> <pre><code>fields: list[qd.models.FieldModel] = [\n    {\n        \"name\": \"indicator_code\",\n        \"title\": \"Indicator Code\",\n        \"type\": \"string\",\n        \"description\": \"World Bank code reference for Indicator Name.\",\n        \"constraints\": {\"required\": True},\n    },\n    {\n        \"name\": \"country_name\",\n        \"title\": \"Country Name\",\n        \"type\": \"string\",\n        \"description\": \"Official country names.\",\n        \"constraints\": {\"required\": True},\n    },\n    {\n        \"name\": \"country_code\",\n        \"title\": \"Country Code\",\n        \"type\": \"string\",\n        \"description\": \"UN ISO 3-letter country code.\",\n        \"constraints\": {\"required\": True},\n    },\n    {\n        \"name\": \"indicator_name\",\n        \"title\": \"Indicator Name\",\n        \"type\": \"string\",\n        \"description\": \"Indicator described in the data series.\",\n        \"constraints\": {\"required\": True},\n    },\n    {\n        \"name\": \"year\",\n        \"title\": \"Year\",\n        \"type\": \"year\",\n        \"description\": \"Year of release.\",\n        \"constraints\": {\"required\": True},\n    },\n    {\n        \"name\": \"values\",\n        \"title\": \"Values\",\n        \"type\": \"number\",\n        \"description\": \"Value for the Year and Indicator Name.\",\n        \"constraints\": {\"required\": True},\n    },\n]\nschema_destination.fields.add_multi(terms=fields)\n</code></pre> <p>This is our curation foundation and we can save it, ensuring citation and version control.</p> <pre><code>schema_destination.save(directory=DIRECTORY, filename=FILENAME, created_by=CREATOR)\n</code></pre> <p>We'll reference this definition as <code>SCHEMA_DESTINATION</code> in the rest of the tutorial.</p>"},{"location":"tutorials/tutorial2/#source-data-and-source-schema-definitions","title":"Source data and source schema definitions","text":"<p>There's a set of challenges when you review the source data:</p> <p></p> <p>The <code>header row</code> doesn't start at <code>index 0</code>. It starts at <code>index 3</code>. Oh, and there are multiple tabs with additional contextual metadata on each. We won't use that in this tutorial, but it's not irrelevant.</p> <p>Reviewing this allows us to derive our source schema as follows:</p> <pre><code>datasource = qd.DataSourceDefinition()\n# There are three sheets:\n# - Data, index 3\n# - Metadata - Countries\n# - Metadata - Indicators\n# We're only going to use the first\ndatasource.derive_model(source=SOURCE_DATA, mimetype=MIMETYPE, header=[3, 0, 0])\nschema_source = qd.SchemaDefinition()\nschema_source.derive_model(data=datasource.get[0])\n</code></pre> <p>Strategy</p> <p>You will have spotted <code>header=[3, 0, 0]</code> defining where the header row can be found. You could even use <code>None</code> if  there is no header row at all and the data start at 0.</p> <p>This isn't some way to skip a crosswalk step. whyqd aims for two key affordances:</p> <ul> <li>Explicit rather than implicit - the software won't automagically derive things without your say-so. It will only   do what you tell it to do.</li> <li>Consistency and predictability - it will always do things the same way each time.</li> </ul> <p>You will need to remove those unnecessary rows in your crosswalk.</p>"},{"location":"tutorials/tutorial2/#defining-crosswalks","title":"Defining crosswalks","text":"<p>Except for that iffy requirement of removing those redundant rows above the header (and that header itself), the  crosswalk is straightforward.</p> <pre><code>crosswalk = qd.CrosswalkDefinition()\ncrosswalk.set(schema_source=schema_source, schema_destination=schema_destination)\n# Create the crosswalk\nschema_scripts = [\n    \"DEBLANK\",\n    \"DEDUPE\",\n    \"DELETE_ROWS &lt; [0, 1, 2, 3]\",\n    f\"PIVOT_LONGER &gt; ['year', 'values'] &lt; {datasource.get[0].names[4:]}\",\n    \"RENAME &gt; 'indicator_code' &lt; ['Indicator Code']\",\n    \"RENAME &gt; 'indicator_name' &lt; ['Indicator Name']\",\n    \"RENAME &gt; 'country_code' &lt; ['Country Code']\",\n    \"RENAME &gt; 'country_name' &lt; ['Country Name']\",\n]\ncrosswalk.actions.add_multi(terms=schema_scripts)\n</code></pre> <p><code>DEBLANK</code> and <code>DEDUPE</code> are useful housekeeping actions, removing all blank rows and columns, and then removing any  completely duplicated rows.</p> <p>The only potentially challenging action is the pivot:</p> <pre><code>f\"PIVOT_LONGER &gt; ['year', 'values'] &lt; {datasource.get[0].names[4:]}\"\n</code></pre> <p>Firstly, the data source model has a <code>names</code> field which is only populated when the header row is to be replaced (i.e. we're not using whatever is at <code>index 0</code> as the header). This is just a list of text terms.</p> <p>We already know the data structure, where the first four columns are 'Country Name', 'Country Code', 'Indicator Name', and 'Indicator Code'. Everything that follows - <code>datasource.get[0].names[4:]</code> - is a <code>year</code> field with <code>values</code> in the rows beneath.</p> <p><code>PIVOT_LONGER</code> transforms each field in the source into terms in two new fields. The first is for the name of the  original field, and the second is for the values beneath it.</p>"},{"location":"tutorials/tutorial2/#transformations-with-crosswalks","title":"Transformations with crosswalks","text":"<p>We can now assign a crosswalk and produce our transformation:</p> <pre><code>transform = qd.TransformDefinition(crosswalk=crosswalk, data_source=datasource.get[0])\ntransform.process()\ntransform.save(directory=DIRECTORY)\n</code></pre> <p>You can validate your transform as well:</p> <pre><code>DESTINATION_DATA = DIRECTORY / transform.model.dataDestination.name\nDESTINATION_MIMETYPE = \"parquet\"\nTRANSFORM = DIRECTORY / f\"{transform.model.name}.transform\"\nvaliform = qd.TransformDefinition()\nvaliform.validate(\n    transform=TRANSFORM, data_destination=DESTINATION_DATA, mimetype_destination=DESTINATION_MIMETYPE\n)\n</code></pre> <p>And your data are ready for archival.</p>"},{"location":"tutorials/tutorial2/#extending-the-tutorial","title":"Extending the tutorial","text":"<p>In the Strategy section, best practice indicates we should derive the country names as categorical terms so that we can support future country-naming variations or changes.</p> <p>This is left to you as an exercise.</p>"},{"location":"tutorials/tutorial3/","title":"Tutorial 3: Wrangling Cthulhu data without losing your mind","text":"<p>whyqd (/w\u026ak\u026ad/) should help you perform crosswalks on any source data. Your challenge is identifying an appropriate strategy.</p> <p>Learning outcomes</p> <ul> <li>Develop techniques for assessing complex source data structure</li> <li>Explain and describe reproducible crosswalks for complex data transformation</li> <li>Perform staged pivot-based crosswalks and generate a schema-compliant output</li> </ul> <p>Source data are from Human Development Report 2007 - 2008. These data are long lost from the internet, and I maintain a repository for educational use. It is assumed you have  familiarity with Python and Pydantic.</p>"},{"location":"tutorials/tutorial3/#background","title":"Background","text":"<p>Quote</p> <p>In 1990 the first Human Development Report introduced a new  approach for advancing human wellbeing. Human development \u2013 or the human development approach - is  about expanding the richness of human life, rather than simply the richness of the economy in which  human beings live. It is an approach that is focused on people and their opportunities and choices.</p> <p>For years I've taught an introductory Data wrangling &amp; validation course using the same training data-series: the Human Development Index report of 2007 - 2008 released by the UNDP as  part of the Millennial Development Goals.</p> <p>The 2007-8 HDI report was listed as a series of about 50 spreadsheets, each dataset aligned with the objectives of the  Millennium Development Goals. These supporting information were used to track  countries meeting the MDG targets.</p> <p>These days it's a slick affair with beautifully-prepared open data in a standardised format. Back then open data was in its infancy, and these data were a constantly-changing mess of non-standard Excel spreadsheets.</p> <p></p> <p>UNDP Human Development Index 2007-2008: a beautiful example of messy data.</p> <p>If you wanted to do any analysis on these data you first had to set about rebuilding these spreadsheets into a single  database aligned to a common schema.</p> <p>The longer you spend with it, the worse it gets. Teaching data curators in countries around the world the importance of  open standards and well-structured machine-readable data is brought home when seeing this and experiencing how difficult  it is to work with.</p> <p>They're a great educational resource, demonstrating that even well-resourced organisations with technically astute  staff are capable of travesties of data curation.</p>"},{"location":"tutorials/tutorial3/#strategy","title":"Strategy","text":"<p>Strategy</p> <p>H.P. Lovecraft, that old scifi-writing bigot, described Cthulhu as:</p> <p>\"A monster of vaguely anthropoid outline, but with an octopus-like head whose face was a mass of feelers, a scaly, rubbery-looking body, prodigious claws on hind and fore feet, and long, narrow wings behind.\"</p> <p>With a monster that big, you don't fight it all at once.</p> <p>The majority of tabular data are stored in spreadsheets on people's desktop computers. For most people, Excel is both database and dashboard visualisation software. That also means that source data are designed, foremost, for  presentation.</p> <p>Such data can have any of:</p> <ul> <li>Merged headers spanning multiple columns and rows</li> <li>Random empty rows and columns</li> <li>Categorical terms defined as sub-headers as data rows instead of as independent fields</li> <li>Joined values containing both quantitative and qualitative data (such as a term and a date)</li> <li>Non-numeric data in numeric fields (such as the multiple ways of showing \"missing\" values)</li> </ul> <p>You'll need to study your source and try and identify all the challenges in your way.</p> <p>The first and most important strategy we can adopt is to recognise that the source is in the wide format. Sure, it's a mess, but we can split our crosswalk in two:</p> <ol> <li>Convert our messy data into a structured wide format</li> <li>Convert the wide- into a standardised long format</li> </ol>"},{"location":"tutorials/tutorial3/#define-a-destination-schema","title":"Define a destination schema","text":"<p>We want our destination data to conform to the following structure:</p> country_name indicator_name reference year values Hong Kong, China (SAR) HDI rank e 2008 21 <p>We just not going to get there all in one go. First we'll need to define an interim destination schema in a wide  format.</p>"},{"location":"tutorials/tutorial3/#interim-destination-wide-format-schema","title":"Interim destination wide-format schema","text":"<p>When you open our tutorial file as a CSV format ... well ... it's not pretty:</p> Unnamed: 0 Unnamed: 1 Unnamed: 2 Monitoring human development: enlarging people's choices \u2026 Unnamed: 4 Unnamed: 5 Unnamed: 6 Unnamed: 7 Unnamed: 8 Unnamed: 9 Unnamed: 10 Unnamed: 11 Unnamed: 12 Unnamed: 13 Unnamed: 14 Unnamed: 15 Unnamed: 16 Unnamed: 17 Unnamed: 18 Unnamed: 19 Unnamed: 20 Unnamed: 21 0 3 Human and income poverty Developing countries nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan 1 nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan 2 nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan 3 nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan 4 nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan 5 nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan 6 nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan 7 nan nan nan nan nan nan nan Probability at birth of not surviving to age 40a,\u2020 nan nan nan nan nan MDG nan MDG nan nan nan nan nan nan (% of cohort) 2000-05 8 nan nan nan nan nan nan nan nan nan Adult illiteracy rateb,\u2020 nan Population not using an improved water source\u2020 nan Children under weight for age\u2020 nan Population below nan nan nan nan nan HPI-1 rank minus income poverty rankc (% aged 15 and older) (%) (% under age 5) income poverty line 1995-2005 2004 1996-2005d (%) 9 nan nan nan Human poverty index                         (HPI-1) nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan <p>There doesn't seem to be any data. At this stage you may be tempted to start hacking at the file directly and see what  you can fix, but our objective is not only clean data, but also an auditable record of how you went from source to final  that can demonstrate the decisions you made, and whether you were able to maintain all the source data.</p> <p>We're going to want to preserve as much as possible, so we're going to define everything as a <code>string</code> type.</p> <p>Notice that headers define a number of concepts simultaneously:</p> <p></p> <p>UNDP Human Development Index 2007-2008: composite header over multiple lines and columns</p> <p>If you unpack this, you get the following set of column headers:</p> <ul> <li>Population below income poverty line (%), at $1 a day for the period 1990-2005</li> <li>An unlabeled reference column</li> <li>Population below income poverty line (%), at $2 a day* for the period 1990-2005</li> <li>An unlabeled reference column</li> <li>Population below income poverty line (%), at the National poverty line for the period 1990-2004</li> <li>An unlabeled reference column</li> </ul> <p>These are spread over multiple merged columns and rows. We can think about this as requiring two steps:</p> <ol> <li>We first need to get all the columns lined up with a proper header row</li> <li>We then split the header definitions into a <code>year</code> and <code>indicator_name</code> field.</li> </ol> <p>We start by defining an interim schema:</p> <pre><code>datasource = qd.DataSourceDefinition()\ndatasource.derive_model(source=SOURCE_DATA, mimetype=MIMETYPE, header=None)\nschema_source = qd.SchemaDefinition()\nschema_source.derive_model(data=datasource.get)\n</code></pre> <p>We explicitly tell whyqd we don't want to use any header row from the data: <code>header=None</code>. Instead you'll simply get a column-indexed set of terms: <code>column_0</code>, <code>column_1</code>, etc. Look at your data, count across, and write scripts.</p> <p>We can aim for the following set of columns. You'll see we created composite header names joined by <code>;;</code>, which we can split later. We also identified category terms used as headers inside the data, and these will be pivoted into a new <code>HDI Category</code> column.</p> <p>We also derived a source schema which we'll refer to as <code>SCHEMA_SOURCE</code> and also our <code>DATASOURCE</code>.</p> <pre><code>NEW_COLUMNS = [\n    \"HDI rank\",\n    \"Country\",\n    \"Human poverty index (HPI-1) - Rank;;2008\",\n    \"Reference 1\",\n    \"Human poverty index (HPI-1) - Value (%);;2008\",\n    \"Probability at birth of not surviving to age 40 (% of cohort);;2000-05\",\n    \"Reference 2\",\n    \"Adult illiteracy rate (% aged 15 and older);;1995-2005\",\n    \"Reference 3\",\n    \"Population not using an improved water source (%);;2004\",\n    \"Reference 4\",\n    \"Children under weight for age (% under age 5);;1996-2005\",\n    \"Reference 5\",\n    \"Population below income poverty line (%) - $1 a day;;1990-2005\",\n    \"Reference 6\",\n    \"Population below income poverty line (%) - $2 a day;;1990-2005\",\n    \"Reference 7\",\n    \"Population below income poverty line (%) - National poverty line;;1990-2004\",\n    \"Reference 8\",\n    \"HPI-1 rank minus income poverty rank;;2008\",\n    \"HDI Category\",\n]\n</code></pre> <p>Now we define our interim destination schema:</p> <pre><code>schema: qd.models.SchemaModel = {\n    \"name\": \"human-development-report-interim\",\n    \"title\": \"UN Human Development Report 2007 - 2008\",\n    \"description\": \"\"\"\n        In 1990 the first Human Development Report introduced a new approach for\n        advancing human wellbeing. Human development \u2013 or the human development approach - is about\n        expanding the richness of human life, rather than simply the richness of the economy in which\n        human beings live. It is an approach that is focused on people and their opportunities and choices.\"\"\",\n}\nfields: list[qd.models.FieldModel] = [{\"name\": c, \"type\": \"string\"} for c in NEW_COLUMNS]\nschema_interim = qd.SchemaDefinition()\nschema_interim.set(schema=schema)\nschema_interim.fields.add_multi(terms=fields)\n</code></pre> <p>We'll call this definition as <code>SCHEMA_INTERIM</code> below.</p>"},{"location":"tutorials/tutorial3/#final-destination-long-format-schema","title":"Final destination long-format schema","text":"<p>While there will be some slight restructuring, our destination is easy to define. Note the subtle <code>name</code> change in the  schema definition:</p> <pre><code>schema: qd.models.SchemaModel = {\n    \"name\": \"human-development-report\",\n    \"title\": \"UN Human Development Report 2007 - 2008\",\n    \"description\": \"\"\"\n        In 1990 the first Human Development Report introduced a new approach for\n        advancing human wellbeing. Human development \u2013 or the human development approach - is about\n        expanding the richness of human life, rather than simply the richness of the economy in which\n        human beings live. It is an approach that is focused on people and their opportunities and choices.\"\"\",\n}\nfields: list[qd.models.FieldModel] = [\n    {\n        \"name\": \"year\",\n        \"title\": \"Year\",\n        \"type\": \"string\",\n        \"description\": \"Year of release.\",\n    },\n    {\n        \"name\": \"country_name\",\n        \"title\": \"Country Name\",\n        \"type\": \"string\",\n        \"description\": \"Official country names.\",\n        \"constraints\": {\"required\": True},\n    },\n    {\n        \"name\": \"indicator_name\",\n        \"title\": \"Indicator Name\",\n        \"type\": \"string\",\n        \"description\": \"Indicator described in the data series.\",\n    },\n    {\n        \"name\": \"values\",\n        \"title\": \"Values\",\n        \"type\": \"number\",\n        \"description\": \"Value for the Year and Indicator Name.\",\n        \"constraints\": {\"required\": True},\n    },\n    {\n        \"name\": \"reference\",\n        \"title\": \"Reference\",\n        \"type\": \"string\",\n        \"description\": \"Reference to data source.\",\n    },\n]\nschema_destination = qd.SchemaDefinition()\nschema_destination.set(schema=schema)\nschema_destination.fields.add_multi(terms=fields)\n</code></pre> <p>We'll call this definition as <code>SCHEMA_DESTINATION</code> below.</p>"},{"location":"tutorials/tutorial3/#defining-crosswalks-and-transforms","title":"Defining crosswalks and transforms","text":"<p>We're going to do this in one, splitting out the interim steps.</p>"},{"location":"tutorials/tutorial3/#interim-crosswalk-and-transform","title":"Interim crosswalk and transform","text":"<p>Here's a list of the various things we need to fix in this first step:</p> <ul> <li>Replace the <code>nan</code> headers with a proper text header row,</li> <li>Convert the row-level categories (at rows 15, 45 and 121) into actual categories,</li> <li>Remove unnecessary rows towards the bottom of the table (from 144 onwards),</li> <li>Rename the default generated columns to the interim schema.</li> </ul> <p>The information we're deleting from the bottom is also not irrelevent. They're the references that the abbreviated terms  in <code>reference</code> refer to (<code>e</code> is a reference to the footnotes). However, we can extract these footnotes in a separate  process.</p> <pre><code>crosswalk_interim = qd.CrosswalkDefinition()\ncrosswalk_interim.set(schema_source=SCHEMA_SOURCE, schema_destination=SCHEMA_INTERIM)\n# Create the crosswalk\nschema_scripts = [\n    f\"DELETE_ROWS &lt; {list(range(15)) + list(np.arange(144, SCHEMA_SOURCE.get.index))}\",\n    \"PIVOT_CATEGORIES &gt; 'HDI Category' &lt; 'column_0'::[15, 45, 121]\",\n]\nindices = [0, 1, 3, 4, 5, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21]\nfor i, j in enumerate(indices):\n    source_field = SCHEMA_SOURCE.fields.get_all()[j]\n    interim_field = SCHEMA_INTERIM.fields.get_all()[i]\n    schema_scripts.append(f\"RENAME &gt; '{interim_field.name}' &lt; ['{source_field.name}']\")\ncrosswalk_interim.actions.add_multi(terms=schema_scripts)\n</code></pre> <p>We're using <code>DELETE_ROWS</code> to delete two ranges, from the top and bottom.</p> <p><code>PIVOT_CATEGORIES</code> requires a bit of thinking. We have categorical headers at indexed rows 15, 45 and 121. There's no other information in these rows, and these headings categorise all the data in the following rows. We're going to pivot these categories into their own field, <code>HDI Category</code>.</p> <p>All of that captured in the following script: <code>\"PIVOT_CATEGORIES &gt; 'HDI Category' &lt; 'column_0'::[15, 45, 121]\"</code></p> <p>Next is just a bity of Python optimisation for the <code>RENAME</code> step. Some of the source data columns are empty and we will discard them. All remaining fields in the schema - except for <code>HDI Category</code> - can be processed by aligning the indices and adding them as part of the crosswalk actions.</p> <p>We can implement the transform and see what we got:</p> <pre><code>transform_interim = qd.TransformDefinition(crosswalk=CROSSWALK_INTERIM, data_source=DATASOURCE.get)\ntransform_interim.process()\ntransform_interim.save(directory=DIRECTORY)\n</code></pre> <p>We save it since we'll need the interim data as an import.</p> <p></p> <p>UNDP Human Development Index 2007-2008: interim crosswalked data outpu</p> <p>Looks a lot better. And you can see how nicely lined-up <code>HDI Category</code> data.</p>"},{"location":"tutorials/tutorial3/#destination-crosswalk-and-transform","title":"Destination crosswalk and transform","text":"<p>This leaves the following to be done in the destination crosswalk:</p> <ul> <li>Pivot the header row indicator terms to create a new 'indicator_name' column,</li> <li>Split the 'indicator_name' column to separate the 'year' into its own column,</li> <li>Join all the separate 'Reference' columns into a single column.</li> </ul> <pre><code>SCHEMA_INTERIM.get.index = transform.get.dataDestination.index\ncrosswalk_destination = qd.CrosswalkDefinition()\ncrosswalk_destination.set(schema_source=SCHEMA_INTERIM, schema_destination=SCHEMA_DESTINATION)\n# Create the crosswalk\nreference_columns = [c.name for c in SCHEMA_INTERIM.fields.get_all() if c.name.startswith(\"Reference\")]\nschema_scripts = [\n    f\"UNITE &gt; 'reference' &lt; {reference_columns}\",\n    \"RENAME &gt; 'country_name' &lt; ['Country']\",\n    \"PIVOT_LONGER &gt; ['indicator_name', 'values'] &lt; ['HDI rank', 'HDI Category', 'Human poverty index (HPI-1) - Rank;;2008', 'Human poverty index (HPI-1) - Value (%);;2008', 'Probability at birth of not surviving to age 40 (% of cohort);;2000-05', 'Adult illiteracy rate (% aged 15 and older);;1995-2005', 'Population not using an improved water source (%);;2004', 'Children under weight for age (% under age 5);;1996-2005', 'Population below income poverty line (%) - $1 a day;;1990-2005', 'Population below income poverty line (%) - $2 a day;;1990-2005', 'Population below income poverty line (%) - National poverty line;;1990-2004', 'HPI-1 rank minus income poverty rank;;2008']\",\n    \"SEPARATE &gt; ['indicator_name', 'year'] &lt; ';;'::['indicator_name']\",\n    \"DEBLANK\",\n    \"DEDUPE\",\n]\ncrosswalk_destination.actions.add_multi(terms=schema_scripts)\n</code></pre> <p><code>UNITE</code> will join all the string terms in the <code>reference_columns</code> into the <code>reference</code> field. By default, this is comma-separated: <code>e, f, g</code>.</p> <p><code>PIVOT_LONGER</code> you've seen before in the second tutorial, but simply pivots all the wide-format headers into an <code>indicator_name</code> term, and their corresponding <code>values</code>.</p> <p>But ... we set up a clever twist for ourselves. Some of the new <code>indicator_name</code> terms can be split to extract the <code>year</code> terms. We use <code>SEPARATE</code>, splitting the text on <code>;;</code>.  Where terms have no <code>;;</code>, the <code>year</code> column will remain null.</p> <p>Finally, one last thing to observe ... <code>SCHEMA_INTERIM.get.index = transform.get.dataDestination.index</code></p> <p>The interim schema doesn't have an index count, but we're doing a lot of physical transformation, so we need to explicitly pass this index from the transform to ensure that the destination transform remains within bounds.</p> <pre><code>transform = qd.TransformDefinition(crosswalk=crosswalk, data_source=transform_interim.get.dataDestination)\ntransform.process()\ntransform.save(directory=DIRECTORY)\n</code></pre> <p>And our output:</p> country_name indicator_name reference year values Hong Kong, China (SAR) HDI rank e 2008 21 Singapore HDI rank nan 2008 25 Korea (Republic of) HDI rank nan 2008 26 Cyprus HDI rank nan 2008 28 Brunei Darussalam HDI rank nan 2008 30"},{"location":"tutorials/tutorial3/#extending-the-tutorial","title":"Extending the tutorial","text":"<p>I encourage you to explore this dataset and see if you agree with the decisions made. And, hopefully, as far as Cthulhu datasets are concerned, the deep holds a smidgeon fewer fears.</p>"}]}